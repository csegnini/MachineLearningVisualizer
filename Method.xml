<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Methods xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
	<Method>
		<Id>1</Id>
		<Node>10</Node>
		<Lvl>3</Lvl>
		<From>6</From>
		<Name>Sampling (Rationale for splitting)</Name>
		<Description>The process of selecting a subset of data from a larger dataset to train, validate, and test machine learning models, with the rationale focusing on creating representative and unbiased subsets.</Description>
		<InputData>Dataset, splitting criteria</InputData>
		<OutputData>Subsets of the dataset (e.g., training, validation, test sets)</OutputData>
		<WhenToUse>When dividing a dataset for machine learning model development and evaluation; to ensure representative training, validation, and test sets.</WhenToUse>
		<Strengths>Ensures representative datasets, reduces bias, enables robust model evaluation.</Strengths>
		<Limitations>Potential for bias if not done carefully, risk of unrepresentative subsets, can be computationally expensive for very large datasets.</Limitations>
		<RelationshipToModels>Essential for training, validating, and testing any machine learning model; directly impacts model generalization.</RelationshipToModels>
		<EvaluationMetrics>Metrics used to evaluate the *models* trained on the sampled data (e.g., accuracy, RMSE). The sampling method itself is evaluated by how well it facilitates robust model evaluation.</EvaluationMetrics>
		<Tags>Data Splitting, Evaluation, Generalization</Tags>
	</Method>
	<Method>
		<Id>2</Id>
		<Node>17</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Data Preprocessing (Flow)</Name>
		<Description>The sequence of steps applied to raw data to transform it into a suitable format for machine learning, including cleaning, transformation, and feature engineering.</Description>
		<InputData>Raw dataset, preprocessing steps</InputData>
		<OutputData>Cleaned and transformed dataset</OutputData>
		<WhenToUse>At the beginning of any machine learning project; to clean, transform, and prepare raw data before model training.</WhenToUse>
		<Strengths>Improves data quality, enhances model accuracy, handles missing data, transforms data for compatibility.</Strengths>
		<Limitations>Can introduce complexity, risk of information loss, potential for data leakage if not done correctly.</Limitations>
		<RelationshipToModels>A necessary preliminary step for all machine learning models; prepares data for model consumption.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance (e.g., increased accuracy, reduced error). Quality metrics of the preprocessing steps (e.g., percentage of missing values imputed).</EvaluationMetrics>
		<Tags>Data Cleaning, Transformation, Feature Engineering</Tags>
	</Method>
	<Method>
		<Id>3</Id>
		<Node>18</Node>
		<Lvl>2</Lvl>
		<From>17</From>
		<Name>Imputation</Name>
		<Description>Replacing missing values in a dataset with estimated values to maintain data completeness and enable model training.</Description>
		<InputData>Dataset with missing values</InputData>
		<OutputData>Dataset with imputed values</OutputData>
		<WhenToUse>When a dataset contains missing values; to avoid data loss and bias in model training.</WhenToUse>
		<Strengths>Preserves data completeness, avoids data loss, reduces bias from incomplete cases.</Strengths>
		<Limitations>Can introduce bias, distorts original data distribution, reduces data variability.</Limitations>
		<RelationshipToModels>Used to prepare data for models that cannot handle missing values; influences model accuracy and robustness.</RelationshipToModels>
		<EvaluationMetrics>Comparison of the imputed data distribution to the original data distribution. Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Missing Data, Data Handling</Tags>
	</Method>
	<Method>
		<Id>4</Id>
		<Node>19</Node>
		<Lvl>3</Lvl>
		<From>18</From>
		<Name>Label Imputation</Name>
		<Description>Specifically imputing missing values in the target variable or class labels of a dataset.</Description>
		<InputData>Dataset with missing labels</InputData>
		<OutputData>Dataset with imputed labels</OutputData>
		<WhenToUse>When target variables or class labels are missing; to enable supervised learning algorithms.</WhenToUse>
		<Strengths>Enables the use of supervised learning algorithms with incomplete labels, maximizes training data.</Strengths>
		<Limitations>High risk of introducing bias into the target variable, can lead to inaccurate model evaluation.</Limitations>
		<RelationshipToModels>Enables training of supervised learning models when labels are incomplete.</RelationshipToModels>
		<EvaluationMetrics>Accuracy of the imputed labels (if ground truth is available for comparison). Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Missing Data, Supervised Learning</Tags>
	</Method>
	<Method>
		<Id>5</Id>
		<Node>20</Node>
		<Lvl>4</Lvl>
		<From>19</From>
		<Name>Direct</Name>
		<Description>An imputation method that uses observed values of the variable itself to estimate missing values.</Description>
		<InputData>Variable with missing values</InputData>
		<OutputData>Imputed variable</OutputData>
		<WhenToUse>When missing values can be reasonably estimated from other observed values of the same variable.</WhenToUse>
		<Strengths>Simple to implement, preserves original data distribution (if applicable).</Strengths>
		<Limitations>May not be accurate if the variable has complex relationships, can be sensitive to outliers.</Limitations>
		<RelationshipToModels>Used to prepare data for models that cannot handle missing values; influences model accuracy and robustness.</RelationshipToModels>
		<EvaluationMetrics>Accuracy of imputed values compared to actual values (if available).</EvaluationMetrics>
		<Tags>Imputation, Data Handling</Tags>
	</Method>
	<Method>
		<Id>6</Id>
		<Node>21</Node>
		<Lvl>4</Lvl>
		<From>19</From>
		<Name>Indirect/Proxy</Name>
		<Description>An imputation method that uses values from other variables to estimate missing values.</Description>
		<InputData>Variable with missing values, related variables</InputData>
		<OutputData>Imputed variable</OutputData>
		<WhenToUse>When missing values can be more accurately estimated using information from related variables.</WhenToUse>
		<Strengths>Potentially more accurate estimates by leveraging relationships with other variables.</Strengths>
		<Limitations>Relies on the accuracy and relevance of proxy variables, can introduce noise from other variables.</Limitations>
		<RelationshipToModels>Used to prepare data for models that cannot handle missing values; influences model accuracy and robustness.</RelationshipToModels>
		<EvaluationMetrics>Accuracy of imputed values compared to actual values (if available). Correlation between imputed values and proxy variables.</EvaluationMetrics>
		<Tags>Imputation, Data Handling</Tags>
	</Method>
	<Method>
		<Id>7</Id>
		<Node>22</Node>
		<Lvl>3</Lvl>
		<From>18</From>
		<Name>Feature Imputation</Name>
		<Description>Imputing missing values in the independent variables or features of a dataset.</Description>
		<InputData>Dataset with missing feature values</InputData>
		<OutputData>Dataset with imputed feature values</OutputData>
		<WhenToUse>When independent variables have missing values; to preserve data points and improve model robustness.</WhenToUse>
		<Strengths>Maintains feature completeness, allows models to utilize all available information.</Strengths>
		<Limitations>Can distort feature relationships, may not be suitable for all types of missing data.</Limitations>
		<RelationshipToModels>Used to prepare data for models that cannot handle missing values; influences model accuracy and robustness.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Imputation, Feature Engineering</Tags>
	</Method>
	<Method>
		<Id>8</Id>
		<Node>23</Node>
		<Lvl>2</Lvl>
		<From>17</From>
		<Name>Handling Imbalanced Data</Name>
		<Description>Techniques used to address datasets where the classes are not represented equally, aiming to improve model performance on minority classes.</Description>
		<InputData>Imbalanced dataset</InputData>
		<OutputData>Modified dataset or trained model</OutputData>
		<WhenToUse>When the classes in a classification dataset are significantly unevenly distributed; to prevent models from being biased towards the majority class.</WhenToUse>
		<Strengths>Improves model performance on minority classes, prevents bias towards majority class.</Strengths>
		<Limitations>Can lead to information loss or overfitting if not applied carefully.</Limitations>
		<RelationshipToModels>Modifies training data to improve model performance on minority classes; affects model bias and sensitivity.</RelationshipToModels>
		<EvaluationMetrics>Precision, recall, F1-score, AUC-ROC (especially for minority class), geometric mean.</EvaluationMetrics>
		<Tags>Imbalanced Data, Classification</Tags>
	</Method>
	<Method>
		<Id>9</Id>
		<Node>26</Node>
		<Lvl>3</Lvl>
		<From>23</From>
		<Name>Techniques</Name>
		<Description>Refers to the various methods employed to handle imbalanced data.</Description>
		<InputData>Imbalanced dataset</InputData>
		<OutputData>Modified dataset or trained model</OutputData>
		<WhenToUse>Generally, whenever 'Method-8-Handling Imbalanced Data' is needed. This entry is too general and might need a more specific context.</WhenToUse>
		<Strengths>This entry is too general and requires more specific context to define strengths.</Strengths>
		<Limitations>This entry is too general and requires more specific context to define limitations.</Limitations>
		<RelationshipToModels>This entry is too general and requires more specific context to define relationships to models.</RelationshipToModels>
		<EvaluationMetrics>This entry is too general. Refer to the specific imbalanced data handling technique.</EvaluationMetrics>
		<Tags>Imbalanced Data (This is too general, needs context)</Tags>
	</Method>
	<Method>
		<Id>10</Id>
		<Node>27</Node>
		<Lvl>4</Lvl>
		<From>26</From>
		<Name>Down sampling</Name>
		<Description>Reducing the size of the majority class in an imbalanced dataset by randomly removing samples.</Description>
		<InputData>Imbalanced dataset</InputData>
		<OutputData>Downsampled dataset</OutputData>
		<WhenToUse>When the majority class is significantly larger and reducing its size is computationally efficient.</WhenToUse>
		<Strengths>Simple to implement, reduces computational cost and memory usage.</Strengths>
		<Limitations>Leads to information loss, may discard valuable data from the majority class.</Limitations>
		<RelationshipToModels>Modifies training data to improve model performance on minority classes; affects model bias and sensitivity.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Imbalanced Data, Data Reduction</Tags>
	</Method>
	<Method>
		<Id>11</Id>
		<Node>28</Node>
		<Lvl>4</Lvl>
		<From>26</From>
		<Name>Upweighting</Name>
		<Description>Assigning higher weights to the minority class during model training to increase its importance.</Description>
		<InputData>Imbalanced dataset, model</InputData>
		<OutputData>Trained model with adjusted weights</OutputData>
		<WhenToUse>When it's important to give more emphasis to the minority class during model training.</WhenToUse>
		<Strengths>Simple to implement, gives more importance to minority class.</Strengths>
		<Limitations>Can increase the impact of noise in the minority class, may not generalize well.</Limitations>
		<RelationshipToModels>Modifies training data to improve model performance on minority classes; affects model bias and sensitivity.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance, class-weighted metrics.</EvaluationMetrics>
		<Tags>Imbalanced Data, Model Training</Tags>
	</Method>
	<Method>
		<Id>12</Id>
		<Node>29</Node>
		<Lvl>4</Lvl>
		<From>26</From>
		<Name>Rebalance Ratios</Name>
		<Description>Adjusting the class distribution in a dataset, often through a combination of oversampling and undersampling.</Description>
		<InputData>Imbalanced dataset</InputData>
		<OutputData>Rebalanced dataset</OutputData>
		<WhenToUse>When a balanced class distribution is desired for optimal model performance.</WhenToUse>
		<Strengths>Provides flexibility in adjusting class distribution.</Strengths>
		<Limitations>Can distort the original class distribution, may not reflect real-world scenarios.</Limitations>
		<RelationshipToModels>Modifies training data to improve model performance on minority classes; affects model bias and sensitivity.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Imbalanced Data, Data Modification</Tags>
	</Method>
	<Method>
		<Id>13</Id>
		<Node>34</Node>
		<Lvl>3</Lvl>
		<From>30</From>
		<Name>Regularization</Name>
		<Description>Techniques that add a penalty to the model's loss function to prevent overfitting.</Description>
		<InputData>Model, loss function</InputData>
		<OutputData>Regularized model</OutputData>
		<WhenToUse>To prevent overfitting in machine learning models, especially when dealing with high-dimensional data.</WhenToUse>
		<Strengths>Prevents overfitting, improves model generalization, enhances stability.</Strengths>
		<Limitations>Requires careful hyperparameter tuning, can underfit if regularization is too strong.</Limitations>
		<RelationshipToModels>A technique applied during model training to prevent overfitting; influences model complexity and generalization.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance (e.g., accuracy, RMSE) on a validation or test set.</EvaluationMetrics>
		<Tags>Overfitting, Model Complexity</Tags>
	</Method>
	<Method>
		<Id>14</Id>
		<Node>35</Node>
		<Lvl>4</Lvl>
		<From>34</From>
		<Name>L1</Name>
		<Description>L1 regularization (Lasso), which adds a penalty proportional to the absolute value of the coefficients.</Description>
		<InputData>Model, loss function</InputData>
		<OutputData>Regularized model (Lasso)</OutputData>
		<WhenToUse>When feature selection is desired, and a sparse model is preferred.</WhenToUse>
		<Strengths>Performs feature selection, leads to sparse models, improves interpretability.</Strengths>
		<Limitations>Can be computationally expensive, may arbitrarily select one feature from a group of correlated features.</Limitations>
		<RelationshipToModels>A technique applied during model training to prevent overfitting; influences model complexity and generalization.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance and number of selected features.</EvaluationMetrics>
		<Tags>Regularization, Feature Selection, Sparsity</Tags>
	</Method>
	<Method>
		<Id>15</Id>
		<Node>36</Node>
		<Lvl>4</Lvl>
		<From>34</From>
		<Name>L2</Name>
		<Description>L2 regularization (Ridge), which adds a penalty proportional to the square of the coefficients.</Description>
		<InputData>Model, loss function</InputData>
		<OutputData>Regularized model (Ridge)</OutputData>
		<WhenToUse>When multicollinearity is present, and all features are potentially relevant.</WhenToUse>
		<Strengths>Reduces multicollinearity, stabilizes coefficient estimates.</Strengths>
		<Limitations>Does not perform feature selection, all features are retained in the model.</Limitations>
		<RelationshipToModels>A technique applied during model training to prevent overfitting; influences model complexity and generalization.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance (e.g., reduced overfitting).</EvaluationMetrics>
		<Tags>Regularization, Multicollinearity</Tags>
	</Method>
	<Method>
		<Id>16</Id>
		<Node>37</Node>
		<Lvl>4</Lvl>
		<From>34</From>
		<Name>Rate</Name>
		<Description>Context likely refers to a parameter or metric that quantifies change or frequency, but requires further context for a precise definition (e.g., learning rate).</Description>
		<InputData>Data, process (context-dependent)</InputData>
		<OutputData>Quantified rate</OutputData>
		<WhenToUse>This term is too general and requires more context. It could refer to various situations (e.g., learning rate in optimization).</WhenToUse>
		<Strengths>This term is too general and requires more specific context to define strengths.</Strengths>
		<Limitations>This term is too general and requires more specific context to define limitations.</Limitations>
		<RelationshipToModels>This term is too general and requires more specific context to define relationships to models (e.g., learning rate in optimization).</RelationshipToModels>
		<EvaluationMetrics>This term is too general. The evaluation metric depends entirely on what 'rate' refers to (e.g., error rate, conversion rate).</EvaluationMetrics>
		<Tags>Depends on context (e.g., Time Series, Optimization)</Tags>
	</Method>
	<Method>
		<Id>17</Id>
		<Node>56</Node>
		<Lvl>3</Lvl>
		<From>55</From>
		<Name>Numerical Data</Name>
		<Description>Data that represents measurable quantities, as opposed to categorical data.</Description>
		<InputData>Data (set of values)</InputData>
		<OutputData>Data identified as numerical</OutputData>
		<WhenToUse>When dealing with data that represents measurable quantities.</WhenToUse>
		<Strengths>Suitable for mathematical operations, many machine learning algorithms require numerical input.</Strengths>
		<Limitations>May not capture complex relationships inherent in non-numerical data.</Limitations>
		<RelationshipToModels>The primary input for most machine learning models; requires preprocessing methods.</RelationshipToModels>
		<EvaluationMetrics>Statistical properties of the data (e.g., mean, variance, distribution).</EvaluationMetrics>
		<Tags>Data Types, Quantitative</Tags>
	</Method>
	<Method>
		<Id>18</Id>
		<Node>57</Node>
		<Lvl>4</Lvl>
		<From>56</From>
		<Name>Scaling</Name>
		<Description>Transforming numerical data to a specific range to prevent features with larger magnitudes from dominating model training.</Description>
		<InputData>Numerical data</InputData>
		<OutputData>Scaled numerical data</OutputData>
		<WhenToUse>When features have different scales, and distance-based algorithms or gradient-based optimization is used.</WhenToUse>
		<Strengths>Ensures fair contribution of features, improves convergence of optimization algorithms.</Strengths>
		<Limitations>Can distort the relative importance of outliers.</Limitations>
		<RelationshipToModels>A preprocessing step for models sensitive to feature scales; affects model convergence and performance.</RelationshipToModels>
		<EvaluationMetrics>Preservation of data relationships, impact on downstream model performance.</EvaluationMetrics>
		<Tags>Data Transformation, Preprocessing</Tags>
	</Method>
	<Method>
		<Id>19</Id>
		<Node>58</Node>
		<Lvl>5</Lvl>
		<From>57</From>
		<Name>Normalization</Name>
		<Description>A scaling technique that adjusts values relative to a norm, often scaling data to a range between 0 and 1.</Description>
		<InputData>Numerical data</InputData>
		<OutputData>Normalized numerical data</OutputData>
		<WhenToUse>When it is important to have all features within a specific range, often [0, 1].</WhenToUse>
		<Strengths>Brings data to a common range, useful for distance-based algorithms.</Strengths>
		<Limitations>Can compress data into a small range, potentially losing information.</Limitations>
		<RelationshipToModels>A preprocessing step for models sensitive to feature scales; affects model convergence and performance.</RelationshipToModels>
		<EvaluationMetrics>Range of the normalized data.</EvaluationMetrics>
		<Tags>Data Transformation, Scaling</Tags>
	</Method>
	<Method>
		<Id>20</Id>
		<Node>59</Node>
		<Lvl>5</Lvl>
		<From>57</From>
		<Name>Linear scaling</Name>
		<Description>Scaling data using a linear transformation, such as min-max scaling.</Description>
		<InputData>Numerical data, scaling range</InputData>
		<OutputData>Linearly scaled data</OutputData>
		<WhenToUse>When a linear transformation is appropriate to bring data to a new range.</WhenToUse>
		<Strengths>Simple, preserves linear relationships.</Strengths>
		<Limitations>Sensitive to outliers, may not preserve non-linear relationships.</Limitations>
		<RelationshipToModels>A preprocessing step for models sensitive to feature scales; affects model convergence and performance.</RelationshipToModels>
		<EvaluationMetrics>Range of the scaled data.</EvaluationMetrics>
		<Tags>Data Transformation, Scaling</Tags>
	</Method>
	<Method>
		<Id>21</Id>
		<Node>60</Node>
		<Lvl>5</Lvl>
		<From>57</From>
		<Name>Z-score scaling</Name>
		<Description>Scaling data so that it has a mean of 0 and a standard deviation of 1.</Description>
		<InputData>Numerical data, mean, standard deviation</InputData>
		<OutputData>Z-score scaled data</OutputData>
		<WhenToUse>When data is approximately normally distributed, and standardization is needed.</WhenToUse>
		<Strengths>Standardizes data, useful for algorithms sensitive to feature distributions.</Strengths>
		<Limitations>Assumes data is approximately normally distributed, sensitive to outliers.</Limitations>
		<RelationshipToModels>A preprocessing step for models sensitive to feature scales; affects model convergence and performance.</RelationshipToModels>
		<EvaluationMetrics>Mean and standard deviation of the scaled data.</EvaluationMetrics>
		<Tags>Data Transformation, Standardization</Tags>
	</Method>
	<Method>
		<Id>22</Id>
		<Node>61</Node>
		<Lvl>5</Lvl>
		<From>57</From>
		<Name>Log scaling</Name>
		<Description>Applying a logarithmic transformation to data, often used to handle skewed distributions.</Description>
		<InputData>Numerical data</InputData>
		<OutputData>Log-transformed data</OutputData>
		<WhenToUse>When dealing with skewed data, and a transformation can make the data more symmetrical.</WhenToUse>
		<Strengths>Handles skewed data, makes distributions more symmetrical.</Strengths>
		<Limitations>Not applicable to zero or negative values, can distort relationships.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Symmetry of the transformed distribution.</EvaluationMetrics>
		<Tags>Data Transformation, Distribution</Tags>
	</Method>
	<Method>
		<Id>23</Id>
		<Node>62</Node>
		<Lvl>4</Lvl>
		<From>56</From>
		<Name>Transformations</Name>
		<Description>Functions applied to data to change its distribution or representation.</Description>
		<InputData>Data, transformation function</InputData>
		<OutputData>Transformed data</OutputData>
		<WhenToUse>When data needs to be reshaped or modified to better suit a model's assumptions.</WhenToUse>
		<Strengths>Adapts data to model assumptions, reveals hidden patterns.</Strengths>
		<Limitations>Can increase complexity, may not be interpretable.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance, statistical properties of the transformed data.</EvaluationMetrics>
		<Tags>Data Modification, Feature Engineering</Tags>
	</Method>
	<Method>
		<Id>24</Id>
		<Node>63</Node>
		<Lvl>5</Lvl>
		<From>62</From>
		<Name>Clipping</Name>
		<Description>Limiting data values to a specified range to handle outliers.</Description>
		<InputData>Data, minimum and maximum values</InputData>
		<OutputData>Clipped data</OutputData>
		<WhenToUse>When outliers need to be constrained to a specific range.</WhenToUse>
		<Strengths>Limits the influence of extreme values.</Strengths>
		<Limitations>Loses information about extreme values.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Number of values clipped.</EvaluationMetrics>
		<Tags>Outlier Handling, Data Cleaning</Tags>
	</Method>
	<Method>
		<Id>25</Id>
		<Node>64</Node>
		<Lvl>5</Lvl>
		<From>62</From>
		<Name>Binning/Bucketing</Name>
		<Description>Grouping continuous data into discrete intervals or bins.</Description>
		<InputData>Continuous data, bin intervals</InputData>
		<OutputData>Binned, bucketed data</OutputData>
		<WhenToUse>When converting continuous data into discrete intervals for better interpretability or to satisfy model requirements.</WhenToUse>
		<Strengths>Simplifies data, handles non-linearities, improves interpretability.</Strengths>
		<Limitations>Can lead to information loss, introduces artificial boundaries.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Distribution of data points across bins.</EvaluationMetrics>
		<Tags>Data Discretization, Feature Engineering</Tags>
	</Method>
	<Method>
		<Id>26</Id>
		<Node>65</Node>
		<Lvl>5</Lvl>
		<From>62</From>
		<Name>Polynomial transforms</Name>
		<Description>Creating new features by raising existing features to various powers.</Description>
		<InputData>Features, polynomial degree</InputData>
		<OutputData>Transformed features</OutputData>
		<WhenToUse>When capturing non-linear relationships between features and the target variable.</WhenToUse>
		<Strengths>Captures non-linear relationships, increases model flexibility.</Strengths>
		<Limitations>Can lead to overfitting, increases dimensionality.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Feature Engineering, Non-Linearity</Tags>
	</Method>
	<Method>
		<Id>27</Id>
		<Node>66</Node>
		<Lvl>5</Lvl>
		<From>62</From>
		<Name>Synthetic features</Name>
		<Description>New features created from existing features to improve model performance.</Description>
		<InputData>Existing features, transformation function</InputData>
		<OutputData>New features</OutputData>
		<WhenToUse>When existing features do not adequately capture the underlying patterns, and new features can improve model accuracy.</WhenToUse>
		<Strengths>Provides new information to the model, improves predictive power.</Strengths>
		<Limitations>Can introduce noise or spurious correlations.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Feature Engineering, Data Augmentation</Tags>
	</Method>
	<Method>
		<Id>28</Id>
		<Node>67</Node>
		<Lvl>4</Lvl>
		<From>56</From>
		<Name>Feature Vector</Name>
		<Description>A numerical representation of an object or data point, often used as input to machine learning models.</Description>
		<InputData>Data point, object</InputData>
		<OutputData>Numerical vector</OutputData>
		<WhenToUse>When representing data points as numerical vectors, suitable for machine learning algorithms.</WhenToUse>
		<Strengths>Enables use of mathematical operations and machine learning algorithms.</Strengths>
		<Limitations>Can lose information about the original structure of the data.</Limitations>
		<RelationshipToModels>The fundamental input representation for many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Relevance to the task, ability to represent information.</EvaluationMetrics>
		<Tags>Data Representation, Machine Learning Input</Tags>
	</Method>
	<Method>
		<Id>29</Id>
		<Node>68</Node>
		<Lvl>4</Lvl>
		<From>56</From>
		<Name>Quantile Bucketing</Name>
		<Description>Binning data based on its quantiles, ensuring an equal number of data points in each bin.</Description>
		<InputData>Continuous data, number of bins</InputData>
		<OutputData>Quantile-binned data</OutputData>
		<WhenToUse>When you want to create bins with an equal number of data points, often for rank-based analysis.</WhenToUse>
		<Strengths>Creates even distributions of data points across bins.</Strengths>
		<Limitations>Can obscure the original data distribution.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Uniformity of data points across bins.</EvaluationMetrics>
		<Tags>Data Discretization, Distribution</Tags>
	</Method>
	<Method>
		<Id>30</Id>
		<Node>69</Node>
		<Lvl>4</Lvl>
		<From>56</From>
		<Name>Scrubbing</Name>
		<Description>Cleaning and filtering data to remove errors, inconsistencies, or unwanted elements.</Description>
		<InputData>Raw data, cleaning rules</InputData>
		<OutputData>Cleaned data</OutputData>
		<WhenToUse>When data contains errors, inconsistencies, or irrelevant information that needs to be removed or corrected.</WhenToUse>
		<Strengths>Improves data accuracy, reduces errors, enhances data reliability.</Strengths>
		<Limitations>Risk of removing valuable information, can introduce bias if not done carefully.</Limitations>
		<RelationshipToModels>A necessary preliminary step for all machine learning models; prepares data for model consumption.</RelationshipToModels>
		<EvaluationMetrics>Number of errors removed, data consistency metrics.</EvaluationMetrics>
		<Tags>Data Cleaning, Data Quality</Tags>
	</Method>
	<Method>
		<Id>31</Id>
		<Node>70</Node>
		<Lvl>3</Lvl>
		<From>55</From>
		<Name>Categorical Data</Name>
		<Description>Data that represents qualities or characteristics, as opposed to numerical data.</Description>
		<InputData>Data (set of values)</InputData>
		<OutputData>Data identified as categorical</OutputData>
		<WhenToUse>When dealing with data that represents qualities or characteristics.</WhenToUse>
		<Strengths>Represents qualities and characteristics, essential for describing the world.</Strengths>
		<Limitations>Requires specific processing for use in many machine learning models.</Limitations>
		<RelationshipToModels>Transforms categorical data into a format suitable for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Appropriateness for the task, information content.</EvaluationMetrics>
		<Tags>Data Types, Qualitative</Tags>
	</Method>
	<Method>
		<Id>32</Id>
		<Node>71</Node>
		<Lvl>4</Lvl>
		<From>70</From>
		<Name>Encoding</Name>
		<Description>Converting categorical data into a numerical format suitable for machine learning models.</Description>
		<InputData>Categorical data</InputData>
		<OutputData>Numerical representation of categorical data</OutputData>
		<WhenToUse>When categorical data needs to be converted into a numerical format for machine learning models.</WhenToUse>
		<Strengths>Allows categorical data to be used in machine learning models.</Strengths>
		<Limitations>Can increase dimensionality, may introduce biases.</Limitations>
		<RelationshipToModels>Transforms categorical data into a format suitable for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance, preservation of information.</EvaluationMetrics>
		<Tags>Data Transformation, Categorical</Tags>
	</Method>
	<Method>
		<Id>33</Id>
		<Node>72</Node>
		<Lvl>5</Lvl>
		<From>71</From>
		<Name>One Hot -Encoding</Name>
		<Description>Representing categorical variables as binary vectors, with a 1 indicating the presence of a category.</Description>
		<InputData>Categorical data</InputData>
		<OutputData>One-hot encoded data</OutputData>
		<WhenToUse>When categorical features are nominal (no inherent order), and each category should be treated as a separate feature.</WhenToUse>
		<Strengths>Avoids ordinality bias, clearly separates categories.</Strengths>
		<Limitations>Increases dimensionality, can lead to the curse of dimensionality.</Limitations>
		<RelationshipToModels>Transforms categorical data into a format suitable for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Number of new features created.</EvaluationMetrics>
		<Tags>Data Encoding, Categorical</Tags>
	</Method>
	<Method>
		<Id>34</Id>
		<Node>73</Node>
		<Lvl>5</Lvl>
		<From>71</From>
		<Name>Embeddings</Name>
		<Description>Dense vector representations of categorical variables that capture semantic relationships.</Description>
		<InputData>Categorical data, model</InputData>
		<OutputData>Embedding vectors</OutputData>
		<WhenToUse>When dealing with high-cardinality categorical data, and you want to capture semantic relationships.</WhenToUse>
		<Strengths>Captures semantic relationships, reduces dimensionality.</Strengths>
		<Limitations>Can be computationally expensive to learn, interpretability can be challenging.</Limitations>
		<RelationshipToModels>Transforms categorical data into a format suitable for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on downstream tasks, semantic similarity measures.</EvaluationMetrics>
		<Tags>Data Representation, Categorical, Neural Networks</Tags>
	</Method>
	<Method>
		<Id>35</Id>
		<Node>74</Node>
		<Lvl>5</Lvl>
		<From>71</From>
		<Name>Hashing</Name>
		<Description>Mapping categorical data to a fixed-size vector using a hash function.</Description>
		<InputData>Categorical data</InputData>
		<OutputData>Hashed representation</OutputData>
		<WhenToUse>When dealing with very high-cardinality categorical data, and you need a memory-efficient representation.</WhenToUse>
		<Strengths>Memory-efficient, fast encoding.</Strengths>
		<Limitations>Can lead to collisions, loss of information.</Limitations>
		<RelationshipToModels>Transforms categorical data into a format suitable for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Collision rate, impact on downstream model performance.</EvaluationMetrics>
		<Tags>Data Encoding, Categorical</Tags>
	</Method>
	<Method>
		<Id>36</Id>
		<Node>75</Node>
		<Lvl>4</Lvl>
		<From>70</From>
		<Name>Feature crosses</Name>
		<Description>Combining two or more features to create new interaction features, often including polynomial terms.</Description>
		<InputData>Features</InputData>
		<OutputData>Crossed features</OutputData>
		<WhenToUse>When interactions between features are important, and polynomial terms can capture non-linear relationships.</WhenToUse>
		<Strengths>Captures feature interactions, increases model expressiveness.</Strengths>
		<Limitations>Increases dimensionality, can lead to overfitting.</Limitations>
		<RelationshipToModels>Used to modify data for model compatibility or to improve feature representations.</RelationshipToModels>
		<EvaluationMetrics>Impact on downstream model performance.</EvaluationMetrics>
		<Tags>Feature Engineering, Interaction</Tags>
	</Method>
	<Method>
		<Id>37</Id>
		<Node>78</Node>
		<Lvl>3</Lvl>
		<From>77</From>
		<Name>Centroid</Name>
		<Description>The center point of a cluster, often calculated as the mean of the data points in the cluster.</Description>
		<InputData>Cluster data points</InputData>
		<OutputData>Centroid coordinates</OutputData>
		<WhenToUse>When representing the center of a cluster or group.</WhenToUse>
		<Strengths>Represents cluster center, simplifies cluster representation.</Strengths>
		<Limitations>May not accurately represent clusters with complex shapes.</Limitations>
		<RelationshipToModels>A concept used in clustering models like K-means and MeanShift.</RelationshipToModels>
		<EvaluationMetrics>Distance to data points in the cluster.</EvaluationMetrics>
		<Tags>Clustering, Representation</Tags>
	</Method>
	<Method>
		<Id>38</Id>
		<Node>79</Node>
		<Lvl>3</Lvl>
		<From>77</From>
		<Name>Density</Name>
		<Description>A measure of the concentration of data points in a given space.</Description>
		<InputData>Spatial data</InputData>
		<OutputData>Density estimate</OutputData>
		<WhenToUse>When analyzing the concentration of data points in a space.</WhenToUse>
		<Strengths>Reveals areas of high concentration, identifies outliers.</Strengths>
		<Limitations>Sensitive to parameter choices, can be computationally expensive.</Limitations>
		<RelationshipToModels>A concept used in density-based clustering models like DBSCAN and HDBSCAN.</RelationshipToModels>
		<EvaluationMetrics>Visualizations, statistical measures of density.</EvaluationMetrics>
		<Tags>Clustering, Data Analysis</Tags>
	</Method>
	<Method>
		<Id>39</Id>
		<Node>80</Node>
		<Lvl>3</Lvl>
		<From>77</From>
		<Name>Distribution</Name>
		<Description>The way in which data values are spread out or arranged.</Description>
		<InputData>Dataset</InputData>
		<OutputData>Statistical description of the data distribution</OutputData>
		<WhenToUse>When describing the statistical properties of a dataset.</WhenToUse>
		<Strengths>Describes data patterns, informs statistical analysis.</Strengths>
		<Limitations>May not fully capture complex data patterns.</Limitations>
		<RelationshipToModels>A concept relevant to various statistical models and influences model assumptions.</RelationshipToModels>
		<EvaluationMetrics>Statistical measures (e.g., skewness, kurtosis).</EvaluationMetrics>
		<Tags>Statistics, Data Analysis</Tags>
	</Method>
	<Method>
		<Id>40</Id>
		<Node>81</Node>
		<Lvl>3</Lvl>
		<From>77</From>
		<Name>Hierarchical</Name>
		<Description>Relating to a structure where data or categories are organized in levels or a tree-like fashion.</Description>
		<InputData>Data to be clustered</InputData>
		<OutputData>Hierarchical clustering structure</OutputData>
		<WhenToUse>When data has a natural hierarchical structure, or a hierarchical grouping is desired.</WhenToUse>
		<Strengths>Reveals relationships at different levels of granularity.</Strengths>
		<Limitations>Can be computationally expensive, sensitive to noise.</Limitations>
		<RelationshipToModels>Describes a structure used in hierarchical clustering models.</RelationshipToModels>
		<EvaluationMetrics>Dendrogram visualization, cophenetic correlation coefficient.</EvaluationMetrics>
		<Tags>Data Structure, Relationships</Tags>
	</Method>
	<Method>
		<Id>41</Id>
		<Node>82</Node>
		<Lvl>2</Lvl>
		<From>76</From>
		<Name>Similarity/Distance Measures</Name>
		<Description>Metrics used to quantify the similarity or dissimilarity between data points.</Description>
		<InputData>Data points or objects</InputData>
		<OutputData>Quantified similarity or distance</OutputData>
		<WhenToUse>When comparing data points or objects based on their attributes.</WhenToUse>
		<Strengths>Quantifies relationships between data points, enables comparisons.</Strengths>
		<Limitations>Choice of metric can significantly impact results, computationally expensive for large datasets.</Limitations>
		<RelationshipToModels>Underlie many machine learning algorithms, especially in clustering and KNN.</RelationshipToModels>
		<EvaluationMetrics>Correlation with human judgments, impact on algorithm performance.</EvaluationMetrics>
		<Tags>Data Comparison, Algorithms</Tags>
	</Method>
	<Method>
		<Id>42</Id>
		<Node>83</Node>
		<Lvl>2</Lvl>
		<From>76</From>
		<Name>Algorithms</Name>
		<Description>Step-by-step procedures used to solve a problem or perform a computation.</Description>
		<InputData>Problem to be solved, data</InputData>
		<OutputData>Solution, computation result</OutputData>
		<WhenToUse>Generally, this term is used broadly, but in this context, it might need specification (e.g., optimization algorithms).</WhenToUse>
		<Strengths>Provides step-by-step solutions, automates processes.</Strengths>
		<Limitations>May be computationally expensive, may not find the optimal solution.</Limitations>
		<RelationshipToModels>The computational procedures that machine learning models implement.</RelationshipToModels>
		<EvaluationMetrics>Computational complexity, time and space requirements.</EvaluationMetrics>
		<Tags>Computation, Problem Solving</Tags>
	</Method>
	<Method>
		<Id>43</Id>
		<Node>84</Node>
		<Lvl>3</Lvl>
		<From>83</From>
		<Name>K-means</Name>
		<Description>A clustering algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean (centroid).</Description>
		<InputData>Data, number of clusters (k)</InputData>
		<OutputData>Cluster assignments</OutputData>
		<WhenToUse>When partitioning data into k distinct clusters based on similarity.</WhenToUse>
		<Strengths>Simple, efficient, widely applicable.</Strengths>
		<Limitations>Requires pre-defining the number of clusters, sensitive to initial centroids.</Limitations>
		<RelationshipToModels>A specific clustering algorithm that defines a type of model.</RelationshipToModels>
		<EvaluationMetrics>Silhouette score, Calinski-Harabasz index, Davies-Bouldin index.</EvaluationMetrics>
		<Tags>Clustering, Unsupervised Learning</Tags>
	</Method>
	<Method>
		<Id>44</Id>
		<Node>85</Node>
		<Lvl>3</Lvl>
		<From>83</From>
		<Name>MeanShift</Name>
		<Description>A clustering algorithm that locates the maxima of density functions, assigning data points to the nearest mode.</Description>
		<InputData>Data, bandwidth</InputData>
		<OutputData>Cluster assignments</OutputData>
		<WhenToUse>When finding clusters based on density gradients, without needing to specify the number of clusters beforehand.</WhenToUse>
		<Strengths>Automatically finds cluster centers, robust to outliers.</Strengths>
		<Limitations>Computationally expensive, sensitive to bandwidth parameter.</Limitations>
		<RelationshipToModels>A specific clustering algorithm that defines a type of model.</RelationshipToModels>
		<EvaluationMetrics>Number of clusters found, stability of cluster assignments.</EvaluationMetrics>
		<Tags>Clustering, Density-Based</Tags>
	</Method>
	<Method>
		<Id>45</Id>
		<Node>86</Node>
		<Lvl>3</Lvl>
		<From>83</From>
		<Name>Spectral</Name>
		<Description>Clustering techniques that use the eigenvalues and eigenvectors of a similarity matrix to partition data.</Description>
		<InputData>Similarity matrix</InputData>
		<OutputData>Cluster assignments</OutputData>
		<WhenToUse>When clusters have complex shapes, and traditional distance-based methods may fail.</WhenToUse>
		<Strengths>Effective for non-convex clusters, utilizes graph theory.</Strengths>
		<Limitations>Computationally expensive for large graphs, sensitive to similarity matrix construction.</Limitations>
		<RelationshipToModels>A specific clustering algorithm that defines a type of model.</RelationshipToModels>
		<EvaluationMetrics>Normalized cuts, modularity.</EvaluationMetrics>
		<Tags>Clustering, Graph Theory</Tags>
	</Method>
	<Method>
		<Id>46</Id>
		<Node>87</Node>
		<Lvl>3</Lvl>
		<From>83</From>
		<Name>HDBSCAN</Name>
		<Description>Density-based clustering algorithm that can discover clusters of varying densities.</Description>
		<InputData>Data, parameters (min_samples, etc.)</InputData>
		<OutputData>Cluster assignments, noise labels</OutputData>
		<WhenToUse>When clusters have varying densities, and noise needs to be handled.</WhenToUse>
		<Strengths>Finds clusters of varying densities, robust to noise.</Strengths>
		<Limitations>Parameter selection can be challenging, may not perform well with high-dimensional data.</Limitations>
		<RelationshipToModels>A specific clustering algorithm that defines a type of model.</RelationshipToModels>
		<EvaluationMetrics>Cluster stability, noise point ratio.</EvaluationMetrics>
		<Tags>Clustering, Density-Based, Noise</Tags>
	</Method>
	<Method>
		<Id>47</Id>
		<Node>88</Node>
		<Lvl>3</Lvl>
		<From>83</From>
		<Name>GaussianMix</Name>
		<Description>A probabilistic model that assumes data points are generated from a mixture of Gaussian distributions.</Description>
		<InputData>Data, number of components</InputData>
		<OutputData>Cluster assignments, probabilities</OutputData>
		<WhenToUse>When data is assumed to be generated from a mixture of Gaussian distributions.</WhenToUse>
		<Strengths>Provides probabilistic cluster assignments, flexible distribution assumptions.</Strengths>
		<Limitations>Sensitive to initialization, can get stuck in local optima.</Limitations>
		<RelationshipToModels>A probabilistic model used for clustering.</RelationshipToModels>
		<EvaluationMetrics>Log-likelihood, BIC, AIC.</EvaluationMetrics>
		<Tags>Clustering, Probabilistic</Tags>
	</Method>
	<Method>
		<Id>48</Id>
		<Node>95</Node>
		<Lvl>2</Lvl>
		<From>76</From>
		<Name>Supervised similarity measure</Name>
		<Description>A similarity measure learned from labeled data to improve performance in supervised tasks.</Description>
		<InputData>Labeled data, data points</InputData>
		<OutputData>Similarity scores</OutputData>
		<WhenToUse>When labeled data is available to learn a similarity metric tailored to a specific task.</WhenToUse>
		<Strengths>Tailored to specific tasks, improves performance in supervised settings.</Strengths>
		<Limitations>Requires labeled data, can overfit if not enough data is available.</Limitations>
		<RelationshipToModels>A similarity measure learned to improve the performance of supervised models.</RelationshipToModels>
		<EvaluationMetrics>Performance on the supervised task (e.g., accuracy, F1-score).</EvaluationMetrics>
		<Tags>Similarity Learning, Supervised</Tags>
	</Method>
	<Method>
		<Id>49</Id>
		<Node>96</Node>
		<Lvl>2</Lvl>
		<From>76</From>
		<Name>Autoencoders</Name>
		<Description>Neural network models that learn efficient data codings in an unsupervised manner.</Description>
		<InputData>Data</InputData>
		<OutputData>Encoded representation</OutputData>
		<WhenToUse>When learning compressed representations of data in an unsupervised manner, often for dimensionality reduction or anomaly detection.</WhenToUse>
		<Strengths>Learns efficient data representations, enables dimensionality reduction and anomaly detection.</Strengths>
		<Limitations>Can be complex to train, may not learn meaningful representations.</Limitations>
		<RelationshipToModels>A type of neural network model used for unsupervised learning.</RelationshipToModels>
		<EvaluationMetrics>Reconstruction error, embedding quality on downstream tasks.</EvaluationMetrics>
		<Tags>Unsupervised Learning, Representation Learning</Tags>
	</Method>
	<Method>
		<Id>50</Id>
		<Node>97</Node>
		<Lvl>2</Lvl>
		<From>76</From>
		<Name>Optimal number of clusters</Name>
		<Description>The ideal number of clusters in a dataset that maximizes separation and minimizes within-cluster variance.</Description>
		<InputData>Data, clustering algorithm</InputData>
		<OutputData>Optimal number of clusters</OutputData>
		<WhenToUse>When determining the most appropriate number of clusters for a given dataset.</WhenToUse>
		<Strengths>Improves clustering performance, avoids overfitting or underfitting.</Strengths>
		<Limitations>Methods can be computationally expensive or unreliable.</Limitations>
		<RelationshipToModels>A parameter selection problem relevant to clustering models.</RelationshipToModels>
		<EvaluationMetrics>Clustering evaluation metrics (e.g., silhouette score) across different numbers of clusters.</EvaluationMetrics>
		<Tags>Clustering, Parameter Selection</Tags>
	</Method>
	<Method>
		<Id>51</Id>
		<Node>21</Node>
		<Lvl>3</Lvl>
		<From>17</From>
		<Name>Optimization</Name>
		<Description>The process of finding the best solution to a problem, often by minimizing a loss function or maximizing an objective function.</Description>
		<InputData>Objective function, constraints</InputData>
		<OutputData>Optimal solution</OutputData>
		<WhenToUse>When finding the best solution to a problem, often by minimizing a loss function or maximizing an objective function.</WhenToUse>
		<Strengths>Finds the best solution to a problem, improves model performance.</Strengths>
		<Limitations>May not find the global optimum, can be computationally expensive.</Limitations>
		<RelationshipToModels>The core process by which machine learning models learn.</RelationshipToModels>
		<EvaluationMetrics>Value of the objective function, convergence rate.</EvaluationMetrics>
		<Tags>Algorithm, Problem Solving</Tags>
	</Method>
	<Method>
		<Id>52</Id>
		<Node>22</Node>
		<Lvl>4</Lvl>
		<From>21</From>
		<Name>Gradient descent</Name>
		<Description>An iterative optimization algorithm that finds the minimum of a function by repeatedly moving in the direction of the steepest descent of the function.</Description>
		<InputData>Function to minimize, initial point</InputData>
		<OutputData>Minimum of the function</OutputData>
		<WhenToUse>When iteratively finding the minimum of a differentiable function.</WhenToUse>
		<Strengths>Simple, widely applicable, efficient for differentiable functions.</Strengths>
		<Limitations>Can get stuck in local minima, slow convergence near the minimum.</Limitations>
		<RelationshipToModels>A fundamental algorithm used to train many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, value of the function at the minimum.</EvaluationMetrics>
		<Tags>Optimization, Algorithm</Tags>
	</Method>
	<Method>
		<Id>53</Id>
		<Node>23</Node>
		<Lvl>5</Lvl>
		<From>22</From>
		<Name>Stochastic Gradient Descent</Name>
		<Description>A variant of gradient descent that updates the model parameters using the gradient of the loss function calculated on a single randomly selected data point or a small subset of data.</Description>
		<InputData>Function to minimize, data batches, initial point</InputData>
		<OutputData>Minimum of the function</OutputData>
		<WhenToUse>When training machine learning models on large datasets, and computational efficiency is crucial.</WhenToUse>
		<Strengths>Faster updates, less memory intensive, escapes local minima.</Strengths>
		<Limitations>Noisy updates can lead to oscillations, requires careful learning rate tuning.</Limitations>
		<RelationshipToModels>A fundamental algorithm used to train many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, generalization performance.</EvaluationMetrics>
		<Tags>Optimization, Algorithm</Tags>
	</Method>
	<Method>
		<Id>54</Id>
		<Node>24</Node>
		<Lvl>3</Lvl>
		<From>17</From>
		<Name>Regularization</Name>
		<Description>Techniques that add a penalty to the model's loss function to prevent overfitting.</Description>
		<InputData>Model, loss function</InputData>
		<OutputData>Regularized model</OutputData>
		<WhenToUse>To prevent overfitting in machine learning models, especially when dealing with high-dimensional data.</WhenToUse>
		<Strengths>Prevents overfitting, improves model generalization, enhances stability.</Strengths>
		<Limitations>Requires careful hyperparameter tuning, can underfit if regularization is too strong.</Limitations>
		<RelationshipToModels>A technique used to improve the generalization of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance (e.g., accuracy, RMSE) on a validation or test set.</EvaluationMetrics>
		<Tags>Overfitting, Model Complexity</Tags>
	</Method>
	<Method>
		<Id>55</Id>
		<Node>45</Node>
		<Lvl>3</Lvl>
		<From>42</From>
		<Name>Regularization (L2, Early Stopping)</Name>
		<Description>Regularization techniques, including L2 regularization and early stopping, used to prevent overfitting in machine learning models.</Description>
		<InputData>Model, training process</InputData>
		<OutputData>Regularized model</OutputData>
		<WhenToUse>Specifically, when using L2 regularization or early stopping to prevent overfitting.</WhenToUse>
		<Strengths>Specifically prevents overfitting through L2 penalty and early stopping.</Strengths>
		<Limitations>Can be difficult to determine the optimal stopping point.</Limitations>
		<RelationshipToModels>Specific regularization techniques applied to machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on a validation set, generalization error.</EvaluationMetrics>
		<Tags>Regularization, Overfitting</Tags>
	</Method>
	<Method>
		<Id>56</Id>
		<Node>60</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Variable Selection</Name>
		<Description>Choosing a subset of relevant variables or features to use in a model.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Subset of selected variables</OutputData>
		<WhenToUse>When reducing the number of input variables in a model to improve interpretability, reduce complexity, or prevent overfitting.</WhenToUse>
		<Strengths>Improves model interpretability, reduces complexity, prevents overfitting.</Strengths>
		<Limitations>Can be computationally expensive, may not find the globally optimal subset.</Limitations>
		<RelationshipToModels>A process used to simplify and improve the performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance, number of selected variables.</EvaluationMetrics>
		<Tags>Feature Selection, Model Simplification</Tags>
	</Method>
	<Method>
		<Id>57</Id>
		<Node>61</Node>
		<Lvl>2</Lvl>
		<From>60</From>
		<Name>Forward Selection</Name>
		<Description>A variable selection method that starts with an empty model and iteratively adds the most significant variable.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Sequence of selected variables</OutputData>
		<WhenToUse>When starting with a small model and iteratively adding variables based on their contribution.</WhenToUse>
		<Strengths>Simple, computationally efficient for adding variables.</Strengths>
		<Limitations>Greedy approach, may miss better variable combinations.</Limitations>
		<RelationshipToModels>A process used to simplify and improve the performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Model performance at each step.</EvaluationMetrics>
		<Tags>Feature Selection, Model Building</Tags>
	</Method>
	<Method>
		<Id>58</Id>
		<Node>62</Node>
		<Lvl>2</Lvl>
		<From>60</From>
		<Name>Backward elimination</Name>
		<Description>A variable selection method that starts with a full model and iteratively removes the least significant variable.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Sequence of removed variables</OutputData>
		<WhenToUse>When starting with a full model and iteratively removing variables that are not statistically significant.</WhenToUse>
		<Strengths>Considers all variables initially, identifies less important ones.</Strengths>
		<Limitations>Computationally expensive for a large number of variables.</Limitations>
		<RelationshipToModels>A process used to simplify and improve the performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Model performance at each step.</EvaluationMetrics>
		<Tags>Feature Selection, Model Reduction</Tags>
	</Method>
	<Method>
		<Id>59</Id>
		<Node>63</Node>
		<Lvl>2</Lvl>
		<From>60</From>
		<Name>Mixed Stepwise</Name>
		<Description>A combination of forward selection and backward elimination for variable selection.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Subset of selected variables</OutputData>
		<WhenToUse>When combining the advantages of forward selection and backward elimination.</WhenToUse>
		<Strengths>Combines strengths of forward and backward selection.</Strengths>
		<Limitations>Compromise between forward and backward selection limitations.</Limitations>
		<RelationshipToModels>A process used to simplify and improve the performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Model performance and model complexity.</EvaluationMetrics>
		<Tags>Feature Selection, Model Optimization</Tags>
	</Method>
	<Method>
		<Id>60</Id>
		<Node>72</Node>
		<Lvl>3</Lvl>
		<From>68</From>
		<Name>Pruning (Regularization)</Name>
		<Description>Reducing the size of a decision tree to prevent overfitting.</Description>
		<InputData>Decision tree</InputData>
		<OutputData>Pruned decision tree</OutputData>
		<WhenToUse>When simplifying a decision tree to improve its generalization performance.</WhenToUse>
		<Strengths>Simplifies decision trees, improves generalization.</Strengths>
		<Limitations>Can be computationally expensive, may prune away useful branches.</Limitations>
		<RelationshipToModels>A regularization technique specific to decision tree models.</RelationshipToModels>
		<EvaluationMetrics>Tree size, accuracy on a validation set.</EvaluationMetrics>
		<Tags>Decision Trees, Regularization</Tags>
	</Method>
	<Method>
		<Id>61</Id>
		<Node>75</Node>
		<Lvl>3</Lvl>
		<From>68</From>
		<Name>Noise and Randomization</Name>
		<Description>Techniques that introduce randomness or noise into the model training process to improve generalization.</Description>
		<InputData>Model, training data</InputData>
		<OutputData>Robust model</OutputData>
		<WhenToUse>When improving model robustness and preventing overfitting.</WhenToUse>
		<Strengths>Increases model robustness, prevents memorization of training data.</Strengths>
		<Limitations>Can slow down training, may not always improve generalization.</Limitations>
		<RelationshipToModels>Techniques used to improve the robustness and generalization of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Model robustness, generalization performance.</EvaluationMetrics>
		<Tags>Robustness, Generalization</Tags>
	</Method>
	<Method>
		<Id>62</Id>
		<Node>77</Node>
		<Lvl>3</Lvl>
		<From>76</From>
		<Name>Bagging(bootstrap aggregation)</Name>
		<Description>An ensemble technique that trains multiple models on bootstrap samples of the data and combines their predictions.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Ensemble of trained models</OutputData>
		<WhenToUse>When reducing variance and improving the stability of machine learning models.</WhenToUse>
		<Strengths>Reduces variance, improves model stability.</Strengths>
		<Limitations>Less interpretable than single models, can be computationally expensive.</Limitations>
		<RelationshipToModels>An ensemble technique that creates multiple models to improve prediction accuracy.</RelationshipToModels>
		<EvaluationMetrics>Reduction in variance, stability of predictions.</EvaluationMetrics>
		<Tags>Ensemble Methods, Variance Reduction</Tags>
	</Method>
	<Method>
		<Id>63</Id>
		<Node>78</Node>
		<Lvl>3</Lvl>
		<From>76</From>
		<Name>Attribute sampling</Name>
		<Description>Randomly selecting a subset of features for each tree in an ensemble method.</Description>
		<InputData>Dataset, model</InputData>
		<OutputData>Ensemble of models trained on feature subsets</OutputData>
		<WhenToUse>When decorrelating trees in an ensemble method to improve performance.</WhenToUse>
		<Strengths>Reduces correlation between trees, improves ensemble diversity.</Strengths>
		<Limitations>Reduces tree correlation but may miss important features.</Limitations>
		<RelationshipToModels>A technique used within ensemble methods to diversify model training.</RelationshipToModels>
		<EvaluationMetrics>Diversity of trees, ensemble performance.</EvaluationMetrics>
		<Tags>Ensemble Methods, Decorrelation</Tags>
	</Method>
	<Method>
		<Id>64</Id>
		<Node>80</Node>
		<Lvl>4</Lvl>
		<From>79</From>
		<Name>Gradient Descent</Name>
		<Description>See 'Method-52-Gradient descent'.</Description>
		<InputData>Function to minimize, initial point</InputData>
		<OutputData>Minimum of the function</OutputData>
		<WhenToUse>When iteratively finding the minimum of a differentiable function.</WhenToUse>
		<Strengths>Simple, widely applicable, efficient for differentiable functions.</Strengths>
		<Limitations>Can get stuck in local minima, slow convergence near the minimum.</Limitations>
		<RelationshipToModels>A fundamental algorithm used to train many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, value of the function at the minimum.</EvaluationMetrics>
		<Tags>Optimization, Algorithm</Tags>
	</Method>
	<Method>
		<Id>65</Id>
		<Node>81</Node>
		<Lvl>4</Lvl>
		<From>79</From>
		<Name>Shrinkage</Name>
		<Description>Reducing the impact of each model's contribution in an ensemble method to prevent overfitting.</Description>
		<InputData>Ensemble of models, learning rate</InputData>
		<OutputData>Combined prediction with reduced weight on individual models</OutputData>
		<WhenToUse>When reducing the impact of individual models in an ensemble to prevent overfitting.</WhenToUse>
		<Strengths>Reduces the impact of noisy or less important models.</Strengths>
		<Limitations>Requires careful learning rate tuning.</Limitations>
		<RelationshipToModels>A technique used in boosting algorithms to reduce model overfitting.</RelationshipToModels>
		<EvaluationMetrics>Impact on model performance, regularization strength.</EvaluationMetrics>
		<Tags>Ensemble Methods, Regularization</Tags>
	</Method>
	<Method>
		<Id>66</Id>
		<Node>82</Node>
		<Lvl>4</Lvl>
		<From>79</From>
		<Name>Optimizations</Name>
		<Description>See 'Method-51-Optimization'.</Description>
		<InputData>Objective function, constraints</InputData>
		<OutputData>Optimal solution</OutputData>
		<WhenToUse>When finding the best solution to a problem, often by minimizing a loss function or maximizing an objective function.</WhenToUse>
		<Strengths>Finds the best solution to a problem, improves model performance.</Strengths>
		<Limitations>May not find the global optimum, can be computationally expensive.</Limitations>
		<RelationshipToModels>The core process by which machine learning models learn.</RelationshipToModels>
		<EvaluationMetrics>Value of the objective function, convergence rate.</EvaluationMetrics>
		<Tags>Algorithm, Problem Solving</Tags>
	</Method>
	<Method>
		<Id>67</Id>
		<Node>83</Node>
		<Lvl>3</Lvl>
		<From>76</From>
		<Name>AdaBoost</Name>
		<Description>An adaptive boosting algorithm that combines multiple weak learners to create a strong learner.</Description>
		<InputData>Dataset, weak learners</InputData>
		<OutputData>Ensemble of weak learners</OutputData>
		<WhenToUse>When combining weak learners to create a strong learner, particularly for classification tasks.</WhenToUse>
		<Strengths>Adaptively focuses on difficult-to-classify instances, combines weak learners effectively.</Strengths>
		<Limitations>Sensitive to noisy data and outliers, can overfit.</Limitations>
		<RelationshipToModels>A boosting algorithm that combines weak learners to create a strong predictive model.</RelationshipToModels>
		<EvaluationMetrics>Classification accuracy, ROC AUC.</EvaluationMetrics>
		<Tags>Ensemble Methods, Boosting</Tags>
	</Method>
	<Method>
		<Id>68</Id>
		<Node>107</Node>
		<Lvl>3</Lvl>
		<From>100</From>
		<Name>Backpropagation</Name>
		<Description>An algorithm used to train neural networks by calculating the gradient of the loss function with respect to the network's weights.</Description>
		<InputData>Neural network, training data</InputData>
		<OutputData>Trained neural network</OutputData>
		<WhenToUse>When training artificial neural networks.</WhenToUse>
		<Strengths>Efficiently trains neural networks, enables learning complex patterns.</Strengths>
		<Limitations>Can get stuck in local minima, requires careful initialization and learning rate tuning.</Limitations>
		<RelationshipToModels>The primary algorithm used to train artificial neural network models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, training loss.</EvaluationMetrics>
		<Tags>Neural Networks, Training</Tags>
	</Method>
	<Method>
		<Id>69</Id>
		<Node>108</Node>
		<Lvl>3</Lvl>
		<From>100</From>
		<Name>Vanishing/Exploding Gradients</Name>
		<Description>Problems encountered during neural network training where gradients become extremely small or large, hindering learning.</Description>
		<InputData>Neural network, training process</InputData>
		<OutputData>Information about gradient behavior</OutputData>
		<WhenToUse>When diagnosing or addressing training instability in deep neural networks.</WhenToUse>
		<Strengths>Highlights training challenges in deep networks.</Strengths>
		<Limitations>Hinders learning in deep networks, requires specialized architectures and techniques.</Limitations>
		<RelationshipToModels>A problem that affects the training of deep neural network models.</RelationshipToModels>
		<EvaluationMetrics>Gradient magnitudes, training stability.</EvaluationMetrics>
		<Tags>Neural Networks, Training, Stability</Tags>
	</Method>
	<Method>
		<Id>70</Id>
		<Node>109</Node>
		<Lvl>3</Lvl>
		<From>100</From>
		<Name>Dropout Regularization</Name>
		<Description>A regularization technique for neural networks that randomly sets a fraction of neurons to zero during training.</Description>
		<InputData>Neural network, training data</InputData>
		<OutputData>Regularized neural network</OutputData>
		<WhenToUse>When regularizing neural networks to prevent overfitting.</WhenToUse>
		<Strengths>Reduces overfitting, improves network generalization.</Strengths>
		<Limitations>Increases training time, optimal dropout rate can be difficult to determine.</Limitations>
		<RelationshipToModels>A regularization technique used in neural network models.</RelationshipToModels>
		<EvaluationMetrics>Performance on a validation set, reduction in overfitting.</EvaluationMetrics>
		<Tags>Neural Networks, Regularization</Tags>
	</Method>
	<Method>
		<Id>71</Id>
		<Node>110</Node>
		<Lvl>2</Lvl>
		<From>93</From>
		<Name>Multiclass Classification</Name>
		<Description>Extending binary classification to handle more than two classes, using techniques like One-vs-All or One-vs-One, often with a softmax activation function.</Description>
		<InputData>Dataset, classification model</InputData>
		<OutputData>Multi-class classification model</OutputData>
		<WhenToUse>When classifying data into more than two categories.</WhenToUse>
		<Strengths>Extends binary classification, handles multiple classes.</Strengths>
		<Limitations>Can be computationally expensive for a large number of classes.</Limitations>
		<RelationshipToModels>Strategies for adapting binary classification models to multi-class problems.</RelationshipToModels>
		<EvaluationMetrics>Multi-class classification metrics (e.g., macro-F1, weighted-F1).</EvaluationMetrics>
		<Tags>Classification, Multi-Class</Tags>
	</Method>
	<Method>
		<Id>72</Id>
		<Node>112</Node>
		<Lvl>3</Lvl>
		<From>111</From>
		<Name>Embedding vector</Name>
		<Description>A numerical representation of a data point in a lower-dimensional space, capturing relationships between data points.</Description>
		<InputData>Data point, model</InputData>
		<OutputData>Numerical vector</OutputData>
		<WhenToUse>When representing categorical data or other entities as numerical vectors.</WhenToUse>
		<Strengths>Captures semantic relationships, reduces dimensionality.</Strengths>
		<Limitations>Learning embeddings can be computationally expensive, embeddings may not be interpretable.</Limitations>
		<RelationshipToModels>A representation of data used as input or output for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on downstream tasks, similarity to other embeddings.</EvaluationMetrics>
		<Tags>Representation Learning, Neural Networks</Tags>
	</Method>
	<Method>
		<Id>73</Id>
		<Node>114</Node>
		<Lvl>3</Lvl>
		<From>111</From>
		<Name>Dimensionality reduction</Name>
		<Description>Reducing the number of features in a dataset while retaining important information.</Description>
		<InputData>High-dimensional data</InputData>
		<OutputData>Lower-dimensional representation</OutputData>
		<WhenToUse>When reducing the number of variables in a dataset while preserving important information.</WhenToUse>
		<Strengths>Simplifies data, reduces noise, improves efficiency.</Strengths>
		<Limitations>Can lose information, choice of method and parameters is crucial.</Limitations>
		<RelationshipToModels>A technique used to simplify data for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Variance explained, information loss.</EvaluationMetrics>
		<Tags>Feature Reduction, Data Transformation</Tags>
	</Method>
	<Method>
		<Id>74</Id>
		<Node>118</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Dimensionality Reduction</Name>
		<Description>See 'Method-73-Dimensionality reduction'.</Description>
		<InputData>High-dimensional data</InputData>
		<OutputData>Lower-dimensional representation</OutputData>
		<WhenToUse>When reducing the number of variables in a dataset while preserving important information.</WhenToUse>
		<Strengths>Simplifies data, reduces noise, improves efficiency.</Strengths>
		<Limitations>Can lose information, choice of method and parameters is crucial.</Limitations>
		<RelationshipToModels>A technique used to simplify data for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Variance explained, information loss.</EvaluationMetrics>
		<Tags>Feature Reduction, Data Transformation</Tags>
	</Method>
	<Method>
		<Id>75</Id>
		<Node>119</Node>
		<Lvl>2</Lvl>
		<From>93</From>
		<Name>PCA</Name>
		<Description>Principal Component Analysis, a dimensionality reduction technique that identifies the principal components of the data.</Description>
		<InputData>Data matrix</InputData>
		<OutputData>Principal components</OutputData>
		<WhenToUse>When reducing dimensionality while preserving the most variance in the data.</WhenToUse>
		<Strengths>Preserves most variance, reduces redundancy.</Strengths>
		<Limitations>Assumes linearity, sensitive to scaling.</Limitations>
		<RelationshipToModels>A technique used to simplify data for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Explained variance ratio.</EvaluationMetrics>
		<Tags>Dimensionality Reduction, Linear Transformation</Tags>
	</Method>
	<Method>
		<Id>76</Id>
		<Node>120</Node>
		<Lvl>2</Lvl>
		<From>93</From>
		<Name>PLS</Name>
		<Description>Partial Least Squares, a dimensionality reduction technique used in regression to handle multicollinearity.</Description>
		<InputData>Predictor and response variables</InputData>
		<OutputData>Latent variables</OutputData>
		<WhenToUse>When dealing with multicollinearity in regression problems.</WhenToUse>
		<Strengths>Handles multicollinearity, focuses on relevant information for prediction.</Strengths>
		<Limitations>Can be complex to interpret, sensitive to variable selection.</Limitations>
		<RelationshipToModels>A technique used to simplify data for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Prediction accuracy, variance explained.</EvaluationMetrics>
		<Tags>Dimensionality Reduction, Regression</Tags>
	</Method>
	<Method>
		<Id>77</Id>
		<Node>3</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>N-grams</Name>
		<Description>Contiguous sequences of n items from a given sequence of text or words.</Description>
		<InputData>Text sequence</InputData>
		<OutputData>Sequence of n-grams</OutputData>
		<WhenToUse>When processing text data, such as in natural language processing tasks.</WhenToUse>
		<Strengths>Captures local word order, useful for text analysis.</Strengths>
		<Limitations>Can lead to high dimensionality, sensitive to text variations.</Limitations>
		<RelationshipToModels>A representation of text data used in natural language processing models.</RelationshipToModels>
		<EvaluationMetrics>Frequency, coverage of text.</EvaluationMetrics>
		<Tags>Natural Language Processing, Text Analysis</Tags>
	</Method>
	<Method>
		<Id>78</Id>
		<Node>4</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Attention Mechanism (Self-attention)</Name>
		<Description>A neural network mechanism that allows the model to weigh the importance of different parts of the input sequence when processing it.</Description>
		<InputData>Input sequence</InputData>
		<OutputData>Attention weights, context vectors</OutputData>
		<WhenToUse>When modeling sequential data, and it's important to weigh the importance of different parts of the sequence.</WhenToUse>
		<Strengths>Focuses on relevant parts of input, enables parallel processing.</Strengths>
		<Limitations>Computationally expensive for long sequences.</Limitations>
		<RelationshipToModels>A component of Transformer models that enables them to process sequential data effectively.</RelationshipToModels>
		<EvaluationMetrics>Performance on downstream tasks, attention weights.</EvaluationMetrics>
		<Tags>Natural Language Processing, Deep Learning, Transformers</Tags>
	</Method>
	<Method>
		<Id>79</Id>
		<Node>7</Node>
		<Lvl>3</Lvl>
		<From>7</From>
		<Name>Encoder</Name>
		<Description>A neural network that converts input data into a different format, often a lower-dimensional representation.</Description>
		<InputData>Input data</InputData>
		<OutputData>Encoded representation</OutputData>
		<WhenToUse>When converting input data into a different representation, often for tasks like machine translation or information retrieval.</WhenToUse>
		<Strengths>Transforms input into a useful representation for downstream tasks.</Strengths>
		<Limitations>Information loss during encoding.</Limitations>
		<RelationshipToModels>A component of sequence-to-sequence models that transforms input data.</RelationshipToModels>
		<EvaluationMetrics>Reconstruction error, quality of encoded representation.</EvaluationMetrics>
		<Tags>Neural Networks, Representation Learning</Tags>
	</Method>
	<Method>
		<Id>80</Id>
		<Node>8</Node>
		<Lvl>3</Lvl>
		<From>7</From>
		<Name>Decoder</Name>
		<Description>A neural network that converts an encoded representation back into a desired output format.</Description>
		<InputData>Encoded representation</InputData>
		<OutputData>Output data</OutputData>
		<WhenToUse>When generating output data from an encoded representation, such as in machine translation or text generation.</WhenToUse>
		<Strengths>Generates output sequences from encoded representations.</Strengths>
		<Limitations>Can struggle with long-range dependencies.</Limitations>
		<RelationshipToModels>A component of sequence-to-sequence models that generates output data.</RelationshipToModels>
		<EvaluationMetrics>Reconstruction quality, fluency of generated output.</EvaluationMetrics>
		<Tags>Neural Networks, Generation</Tags>
	</Method>
	<Method>
		<Id>81</Id>
		<Node>12</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Techniques</Name>
		<Description>Refers to the various methods or approaches.</Description>
		<InputData>Data, task</InputData>
		<OutputData>Result, processed data</OutputData>
		<WhenToUse>This term is too general and requires more context.</WhenToUse>
		<Strengths>This term is too general and requires more specific context to define strengths.</Strengths>
		<Limitations>This term is too general and requires more specific context to define limitations.</Limitations>
		<RelationshipToModels>This term is too general and requires more specific context to define relationships to models.</RelationshipToModels>
		<EvaluationMetrics>This term is too general. Refer to the specific technique being used.</EvaluationMetrics>
		<Tags>This term is too general, needs context</Tags>
	</Method>
	<Method>
		<Id>82</Id>
		<Node>13</Node>
		<Lvl>2</Lvl>
		<From>13</From>
		<Name>Tuning</Name>
		<Description>The process of optimizing model hyperparameters to improve performance.</Description>
		<InputData>Model, hyperparameters, validation data</InputData>
		<OutputData>Optimized hyperparameters</OutputData>
		<WhenToUse>When optimizing model hyperparameters to improve performance.</WhenToUse>
		<Strengths>Improves model performance, optimizes generalization.</Strengths>
		<Limitations>Can be computationally expensive, risk of overfitting to the validation set.</Limitations>
		<RelationshipToModels>The process of optimizing the performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on a validation set, generalization error.</EvaluationMetrics>
		<Tags>Hyperparameter Optimization, Model Selection</Tags>
	</Method>
	<Method>
		<Id>83</Id>
		<Node>14</Node>
		<Lvl>2</Lvl>
		<From>13</From>
		<Name>Distillation</Name>
		<Description>A model compression technique that transfers knowledge from a large, complex model to a smaller, more efficient model.</Description>
		<InputData>Large model, data</InputData>
		<OutputData>Smaller model</OutputData>
		<WhenToUse>When compressing a large model into a smaller one for efficiency.</WhenToUse>
		<Strengths>Creates smaller, faster models, enables deployment in resource-constrained environments.</Strengths>
		<Limitations>Requires a well-trained teacher model, potential loss of accuracy.</Limitations>
		<RelationshipToModels>A technique for compressing machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance of the distilled model, compression ratio.</EvaluationMetrics>
		<Tags>Model Compression, Efficiency</Tags>
	</Method>
	<Method>
		<Id>84</Id>
		<Node>15</Node>
		<Lvl>2</Lvl>
		<From>13</From>
		<Name>Prompt Engineering</Name>
		<Description>Designing effective prompts to elicit desired responses from large language models.</Description>
		<InputData>Large language model, prompt</InputData>
		<OutputData>Model output</OutputData>
		<WhenToUse>When interacting with large language models to elicit desired responses.</WhenToUse>
		<Strengths>Elicits desired responses from large language models, unlocks their capabilities.</Strengths>
		<Limitations>Requires expertise, results can be unpredictable.</Limitations>
		<RelationshipToModels>A technique for interacting with large language models to guide their output.</RelationshipToModels>
		<EvaluationMetrics>Performance of the LLM on the target task, fluency and relevance of output.</EvaluationMetrics>
		<Tags>Large Language Models, Interaction</Tags>
	</Method>
	<Method>
		<Id>85</Id>
		<Node>0</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Model Selection and Evaluation</Name>
		<Description>The process of choosing the best model from a set of candidates and assessing its performance.</Description>
		<InputData>Candidate models, data</InputData>
		<OutputData>Selected model, evaluation metrics</OutputData>
		<WhenToUse>When choosing the best model and assessing its performance.</WhenToUse>
		<Strengths>Chooses the best model, provides reliable performance estimates.</Strengths>
		<Limitations>Risk of bias in evaluation, computationally expensive.</Limitations>
		<RelationshipToModels>The process of choosing and assessing machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance metrics of the selected model, robustness of evaluation.</EvaluationMetrics>
		<Tags>Model Selection, Evaluation</Tags>
	</Method>
	<Method>
		<Id>86</Id>
		<Node>1</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Model Selection</Name>
		<Description>The process of choosing the best model from a set of candidate models.</Description>
		<InputData>Candidate models, data</InputData>
		<OutputData>Selected model</OutputData>
		<WhenToUse>When choosing the best model from a set of candidate models.</WhenToUse>
		<Strengths>Selects the most suitable model for a given task.</Strengths>
		<Limitations>Can be subjective, risk of selecting a suboptimal model.</Limitations>
		<RelationshipToModels>The process of choosing the best model from a set of candidate models.</RelationshipToModels>
		<EvaluationMetrics>Justification for the model choice (e.g., simplicity, performance).</EvaluationMetrics>
		<Tags>Model Selection</Tags>
	</Method>
	<Method>
		<Id>87</Id>
		<Node>2</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Cross-Validation</Name>
		<Description>A technique for evaluating model performance by partitioning the data into multiple subsets for training and validation.</Description>
		<InputData>Model, data</InputData>
		<OutputData>Performance estimates</OutputData>
		<WhenToUse>When obtaining reliable estimates of model performance.</WhenToUse>
		<Strengths>Provides robust performance estimates, reduces overfitting.</Strengths>
		<Limitations>Computationally expensive, especially for large datasets.</Limitations>
		<RelationshipToModels>A technique for evaluating the generalization performance of machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Variance of performance estimates across folds.</EvaluationMetrics>
		<Tags>Model Evaluation, Generalization</Tags>
	</Method>
	<Method>
		<Id>88</Id>
		<Node>3</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Hypothesis Testing</Name>
		<Description>A statistical method used to make decisions or draw inferences about a population based on sample data.</Description>
		<InputData>Sample data, hypothesis</InputData>
		<OutputData>Test statistic, p-value</OutputData>
		<WhenToUse>When making statistical inferences about populations based on sample data.</WhenToUse>
		<Strengths>Provides a framework for making statistically sound conclusions.</Strengths>
		<Limitations>Assumptions must be met, risk of incorrect conclusions.</Limitations>
		<RelationshipToModels>A statistical method used to validate or reject assumptions made by machine learning models.</RelationshipToModels>
		<EvaluationMetrics>P-value, test statistic.</EvaluationMetrics>
		<Tags>Statistical Inference, Validation</Tags>
	</Method>
	<Method>
		<Id>89</Id>
		<Node>4</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Significance</Name>
		<Description>The statistical likelihood that a result is not due to chance.</Description>
		<InputData>Statistical test results</InputData>
		<OutputData>Significance level</OutputData>
		<WhenToUse>When determining if an observed result is likely due to chance.</WhenToUse>
		<Strengths>Quantifies the reliability of results, helps in decision-making.</Strengths>
		<Limitations>Can be misinterpreted, does not imply practical importance.</Limitations>
		<RelationshipToModels>A statistical concept used to assess the reliability of model results.</RelationshipToModels>
		<EvaluationMetrics>Significance level (alpha).</EvaluationMetrics>
		<Tags>Statistical Inference, Interpretation</Tags>
	</Method>
	<Method>
		<Id>90</Id>
		<Node>5</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Bootstrap</Name>
		<Description>A resampling technique used to estimate statistics (e.g., confidence intervals) by sampling with replacement from the original dataset.</Description>
		<InputData>Sample data</InputData>
		<OutputData>Resampled datasets, statistical estimates</OutputData>
		<WhenToUse>When estimating statistics or constructing confidence intervals.</WhenToUse>
		<Strengths>Provides estimates of uncertainty, useful for statistical inference.</Strengths>
		<Limitations>Computationally expensive, results can vary with different samples.</Limitations>
		<RelationshipToModels>A resampling technique used to estimate the uncertainty of model predictions.</RelationshipToModels>
		<EvaluationMetrics>Confidence intervals, standard errors.</EvaluationMetrics>
		<Tags>Statistical Inference, Uncertainty Estimation</Tags>
	</Method>
	<Method>
		<Id>91</Id>
		<Node>5</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Deployment Types</Name>
		<Description>Different ways in which machine learning models can be put into production.</Description>
		<InputData>Trained model, application requirements</InputData>
		<OutputData>Deployment configuration</OutputData>
		<WhenToUse>When deciding how to make a trained model available for use.</WhenToUse>
		<Strengths>Offers flexibility in how models are made available.</Strengths>
		<Limitations>Each type has its own challenges regarding scalability, latency, etc.</Limitations>
		<RelationshipToModels>Different strategies for making machine learning models available for use.</RelationshipToModels>
		<EvaluationMetrics>Latency, throughput, resource utilization.</EvaluationMetrics>
		<Tags>Model Deployment, Production</Tags>
	</Method>
	<Method>
		<Id>92</Id>
		<Node>6</Node>
		<Lvl>2</Lvl>
		<From>6</From>
		<Name>Static</Name>
		<Description>Model deployment where the model is fixed and does not update frequently.</Description>
		<InputData>Trained model</InputData>
		<OutputData>Deployed static model</OutputData>
		<WhenToUse>When the model does not need frequent updates.</WhenToUse>
		<Strengths>Simple to deploy, suitable for models that don't require frequent updates.</Strengths>
		<Limitations>Requires model retraining for changing data patterns.</Limitations>
		<RelationshipToModels>A type of model deployment where the model does not update.</RelationshipToModels>
		<EvaluationMetrics>Model performance over time (drift).</EvaluationMetrics>
		<Tags>Model Deployment</Tags>
	</Method>
	<Method>
		<Id>93</Id>
		<Node>7</Node>
		<Lvl>3</Lvl>
		<From>7</From>
		<Name>Training</Name>
		<Description>The process of teaching a machine learning model to learn patterns from data.</Description>
		<InputData>Model architecture, data</InputData>
		<OutputData>Trained model</OutputData>
		<WhenToUse>When creating a machine learning model.</WhenToUse>
		<Strengths>Enables models to learn from data.</Strengths>
		<Limitations>Can be computationally expensive, requires large datasets.</Limitations>
		<RelationshipToModels>The process of building a machine learning model.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, training loss.</EvaluationMetrics>
		<Tags>Model Building, Learning</Tags>
	</Method>
	<Method>
		<Id>94</Id>
		<Node>8</Node>
		<Lvl>3</Lvl>
		<From>7</From>
		<Name>Inference</Name>
		<Description>The process of using a trained model to make predictions on new data.</Description>
		<InputData>Trained model, new data</InputData>
		<OutputData>Model predictions</OutputData>
		<WhenToUse>When making predictions with a trained machine learning model.</WhenToUse>
		<Strengths>Allows trained models to make predictions.</Strengths>
		<Limitations>Can be computationally expensive, requires efficient hardware.</Limitations>
		<RelationshipToModels>The process of using a trained machine learning model to make predictions.</RelationshipToModels>
		<EvaluationMetrics>Latency, throughput, accuracy.</EvaluationMetrics>
		<Tags>Prediction, Application</Tags>
	</Method>
	<Method>
		<Id>95</Id>
		<Node>9</Node>
		<Lvl>2</Lvl>
		<From>6</From>
		<Name>Dynamic</Name>
		<Description>Model deployment where the model is updated frequently, often in real-time.</Description>
		<InputData>Trained model, data streams</InputData>
		<OutputData>Deployed dynamic model</OutputData>
		<WhenToUse>When the model needs to be updated frequently to adapt to changing data.</WhenToUse>
		<Strengths>Keeps models up-to-date with changing data patterns.</Strengths>
		<Limitations>Complex to implement, requires continuous data monitoring.</Limitations>
		<RelationshipToModels>A type of model deployment where the model is updated automatically.</RelationshipToModels>
		<EvaluationMetrics>Adaptation speed, performance stability.</EvaluationMetrics>
		<Tags>Model Deployment, Adaptation</Tags>
	</Method>
	<Method>
		<Id>96</Id>
		<Node>10</Node>
		<Lvl>3</Lvl>
		<From>10</From>
		<Name>Training</Name>
		<Description>The process of teaching a machine learning model to learn patterns from data.</Description>
		<InputData>Model architecture, data</InputData>
		<OutputData>Trained model</OutputData>
		<WhenToUse>When creating a machine learning model.</WhenToUse>
		<Strengths>Enables models to learn from data.</Strengths>
		<Limitations>Can be computationally expensive, requires large datasets.</Limitations>
		<RelationshipToModels>The process of building a machine learning model.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, training loss.</EvaluationMetrics>
		<Tags>Model Building, Learning</Tags>
	</Method>
	<Method>
		<Id>97</Id>
		<Node>11</Node>
		<Lvl>3</Lvl>
		<From>10</From>
		<Name>Inference</Name>
		<Description>The process of using a trained model to make predictions on new data.</Description>
		<InputData>Trained model, new data</InputData>
		<OutputData>Model predictions</OutputData>
		<WhenToUse>When making predictions with a trained machine learning model.</WhenToUse>
		<Strengths>Allows trained models to make predictions.</Strengths>
		<Limitations>Can be computationally expensive, requires efficient hardware.</Limitations>
		<RelationshipToModels>The process of using a trained machine learning model to make predictions.</RelationshipToModels>
		<EvaluationMetrics>Latency, throughput, accuracy.</EvaluationMetrics>
		<Tags>Prediction, Application</Tags>
	</Method>
	<Method>
		<Id>98</Id>
		<Node>12</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Model Quality and Monitoring</Name>
		<Description>Processes and techniques for ensuring and tracking the performance and reliability of deployed machine learning models.</Description>
		<InputData>Deployed model, data streams, performance metrics</InputData>
		<OutputData>Model health indicators, alerts</OutputData>
		<WhenToUse>When ensuring and tracking the performance and reliability of deployed models.</WhenToUse>
		<Strengths>Ensures models remain accurate and reliable in production.</Strengths>
		<Limitations>Requires robust infrastructure, complex to detect and diagnose issues.</Limitations>
		<RelationshipToModels>The process of ensuring and maintaining the performance of deployed machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Alert frequency, performance degradation rate.</EvaluationMetrics>
		<Tags>Model Management, Reliability</Tags>
	</Method>
	<Method>
		<Id>99</Id>
		<Node>13</Node>
		<Lvl>2</Lvl>
		<From>13</From>
		<Name>Testing</Name>
		<Description>Evaluating the performance of a model on a held-out test set.</Description>
		<InputData>Trained model, test data</InputData>
		<OutputData>Model performance metrics</OutputData>
		<WhenToUse>When assessing the performance of a trained model on unseen data.</WhenToUse>
		<Strengths>Provides an unbiased estimate of model performance on unseen data.</Strengths>
		<Limitations>Requires representative test data, risk of overfitting to the test set.</Limitations>
		<RelationshipToModels>The process of evaluating a machine learning model on unseen data.</RelationshipToModels>
		<EvaluationMetrics>Performance metrics on the test set.</EvaluationMetrics>
		<Tags>Model Evaluation</Tags>
	</Method>
	<Method>
		<Id>100</Id>
		<Node>16</Node>
		<Lvl>2</Lvl>
		<From>13</From>
		<Name>Pipeline monitoring (Importance)</Name>
		<Description>The significance of continuously tracking the performance and health of machine learning pipelines.</Description>
		<InputData>Machine learning pipeline</InputData>
		<OutputData>Insights into pipeline health and performance</OutputData>
		<WhenToUse>When maintaining the health and efficiency of machine learning workflows.</WhenToUse>
		<Strengths>Ensures efficient and reliable machine learning workflows.</Strengths>
		<Limitations>Can be complex to set up and maintain.</Limitations>
		<RelationshipToModels>The importance of tracking the performance and health of machine learning workflows.</RelationshipToModels>
		<EvaluationMetrics>Pipeline uptime, data quality metrics.</EvaluationMetrics>
		<Tags>Workflow Management, Reliability</Tags>
	</Method>
	<Method>
		<Id>101</Id>
		<Node>0</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Optimization</Name>
		<Description>The process of finding the best solution to a problem, often by minimizing a loss function or maximizing an objective function.</Description>
		<InputData>Objective function, constraints</InputData>
		<OutputData>Optimal solution</OutputData>
		<WhenToUse>When finding the best solution to a problem, often by minimizing a loss function or maximizing an objective function.</WhenToUse>
		<Strengths>Finds the best solution to a problem, improves model performance.</Strengths>
		<Limitations>May not find the global optimum, can be computationally expensive.</Limitations>
		<RelationshipToModels>The core process by which machine learning models learn.</RelationshipToModels>
		<EvaluationMetrics>Value of the objective function, convergence rate.</EvaluationMetrics>
		<Tags>Algorithm, Problem Solving</Tags>
	</Method>
	<Method>
		<Id>102</Id>
		<Node>1</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Optimizers (Gradient Descent Variants)</Name>
		<Description>Algorithms that improve the efficiency and effectiveness of the gradient descent optimization process.</Description>
		<InputData>Model, loss function</InputData>
		<OutputData>Optimized model parameters</OutputData>
		<WhenToUse>When training neural networks or other models with gradient-based optimization.</WhenToUse>
		<Strengths>Improves the efficiency and effectiveness of model training.</Strengths>
		<Limitations>Can be sensitive to hyperparameter tuning.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, final performance.</EvaluationMetrics>
		<Tags>Optimization, Neural Networks</Tags>
	</Method>
	<Method>
		<Id>103</Id>
		<Node>2</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Gradient Descent</Name>
		<Description>An iterative optimization algorithm that finds the minimum of a function by repeatedly moving in the direction of the steepest descent of the function.</Description>
		<InputData>Function to minimize, initial point</InputData>
		<OutputData>Minimum of the function</OutputData>
		<WhenToUse>When iteratively finding the minimum of a differentiable function.</WhenToUse>
		<Strengths>Simple, widely applicable, efficient for differentiable functions.</Strengths>
		<Limitations>Can get stuck in local minima, slow convergence near the minimum.</Limitations>
		<RelationshipToModels>A fundamental algorithm used to train many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, value of the function at the minimum.</EvaluationMetrics>
		<Tags>Optimization, Algorithm</Tags>
	</Method>
	<Method>
		<Id>104</Id>
		<Node>3</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Stochastic Gradient Descent</Name>
		<Description>A variant of gradient descent that updates the model parameters using the gradient of the loss function calculated on a single randomly selected data point or a small subset of data.</Description>
		<InputData>Function to minimize, data batches, initial point</InputData>
		<OutputData>Minimum of the function</OutputData>
		<WhenToUse>When training machine learning models on large datasets, and computational efficiency is crucial.</WhenToUse>
		<Strengths>Faster updates, less memory intensive, escapes local minima.</Strengths>
		<Limitations>Noisy updates can lead to oscillations, requires careful learning rate tuning.</Limitations>
		<RelationshipToModels>A fundamental algorithm used to train many machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence rate, generalization performance.</EvaluationMetrics>
		<Tags>Optimization, Algorithm</Tags>
	</Method>
	<Method>
		<Id>105</Id>
		<Node>4</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Adagrad</Name>
		<Description>An adaptive learning rate optimization algorithm that adjusts the learning rate for each parameter based on its historical gradients.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When adapting learning rates based on historical gradients.</WhenToUse>
		<Strengths>Adapts learning rates to feature importance.</Strengths>
		<Limitations>Learning rate can diminish too quickly.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, parameter updates.</EvaluationMetrics>
		<Tags>Optimization, Adaptive Learning Rate</Tags>
	</Method>
	<Method>
		<Id>106</Id>
		<Node>5</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Adadelta</Name>
		<Description>An extension of Adagrad that addresses its diminishing learning rate problem.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When improving Adagrad's diminishing learning rate.</WhenToUse>
		<Strengths>Addresses Adagrad's diminishing learning rate.</Strengths>
		<Limitations>Can be unstable early in training.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, stability.</EvaluationMetrics>
		<Tags>Optimization, Adaptive Learning Rate</Tags>
	</Method>
	<Method>
		<Id>107</Id>
		<Node>6</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>RMSprop</Name>
		<Description>Root Mean Square Propagation, an adaptive learning rate optimization algorithm.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When using adaptive learning rates in neural networks.</WhenToUse>
		<Strengths>Effective for handling oscillating gradients.</Strengths>
		<Limitations>Requires careful learning rate selection.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, stability.</EvaluationMetrics>
		<Tags>Optimization, Adaptive Learning Rate</Tags>
	</Method>
	<Method>
		<Id>108</Id>
		<Node>7</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Adam</Name>
		<Description>Adaptive Moment Estimation, an optimization algorithm that combines the advantages of Adagrad and RMSprop.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When combining the benefits of Adagrad and RMSprop.</WhenToUse>
		<Strengths>Combines advantages of momentum and adaptive learning rates.</Strengths>
		<Limitations>Can sometimes generalize poorly.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed, generalization performance.</EvaluationMetrics>
		<Tags>Optimization, Adaptive Learning Rate</Tags>
	</Method>
	<Method>
		<Id>109</Id>
		<Node>8</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>AdamW</Name>
		<Description>A variant of Adam with decoupled weight decay regularization.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When applying weight decay regularization with Adam.</WhenToUse>
		<Strengths>Decouples weight decay for better regularization.</Strengths>
		<Limitations>Requires tuning of weight decay.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Generalization performance.</EvaluationMetrics>
		<Tags>Optimization, Regularization</Tags>
	</Method>
	<Method>
		<Id>110</Id>
		<Node>9</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Adamax</Name>
		<Description>A variant of Adam based on the infinity norm.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When needing a variant of Adam with different stability properties.</WhenToUse>
		<Strengths>Robust to large gradients.</Strengths>
		<Limitations>Can be unstable in some cases.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Stability, convergence.</EvaluationMetrics>
		<Tags>Optimization, Robustness</Tags>
	</Method>
	<Method>
		<Id>111</Id>
		<Node>10</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Nadam</Name>
		<Description>Nesterov Adaptive Moment Estimation, an optimization algorithm that incorporates Nesterov's accelerated gradient.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When incorporating Nesterov momentum with Adam.</WhenToUse>
		<Strengths>Incorporates Nesterov momentum for faster convergence.</Strengths>
		<Limitations>Increased complexity.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Convergence speed.</EvaluationMetrics>
		<Tags>Optimization, Momentum</Tags>
	</Method>
	<Method>
		<Id>112</Id>
		<Node>11</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Ftrl</Name>
		<Description>Follow-the-Regularized-Leader, an optimization algorithm designed for sparse data.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When dealing with sparse data in online learning.</WhenToUse>
		<Strengths>Efficient for sparse data and online learning.</Strengths>
		<Limitations>Performance can be sensitive to regularization parameters.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on sparse data.</EvaluationMetrics>
		<Tags>Optimization, Sparse Data</Tags>
	</Method>
	<Method>
		<Id>113</Id>
		<Node>12</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Adafactor</Name>
		<Description>An optimization algorithm that reduces memory consumption by approximating the weight matrices.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When reducing memory consumption in large models.</WhenToUse>
		<Strengths>Reduces memory footprint in large models.</Strengths>
		<Limitations>May not be suitable for all model architectures.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Memory usage, convergence.</EvaluationMetrics>
		<Tags>Optimization, Memory Efficiency</Tags>
	</Method>
	<Method>
		<Id>114</Id>
		<Node>13</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Lion</Name>
		<Description>A novel optimizer that demonstrates improved performance in various deep learning tasks.</Description>
		<InputData>Model parameters, gradients</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When seeking a potentially more effective optimizer than Adam.</WhenToUse>
		<Strengths>Potentially offers improved optimization performance.</Strengths>
		<Limitations>Relatively new, requires further research.</Limitations>
		<RelationshipToModels>Algorithms used to train machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Performance on various tasks.</EvaluationMetrics>
		<Tags>Optimization</Tags>
	</Method>
	<Method>
		<Id>115</Id>
		<Node>14</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>LossScaleOptimizer</Name>
		<Description>An optimizer that scales the loss to avoid underflow or overflow issues during training.</Description>
		<InputData>Model, gradients, loss scale</InputData>
		<OutputData>Updated model parameters</OutputData>
		<WhenToUse>When training models with mixed precision to prevent underflow/overflow.</WhenToUse>
		<Strengths>Enables mixed precision training.</Strengths>
		<Limitations>Requires careful loss scaling management.</Limitations>
		<RelationshipToModels>A modification to optimizers for mixed precision training.</RelationshipToModels>
		<EvaluationMetrics>Training stability, performance.</EvaluationMetrics>
		<Tags>Optimization, Numerical Stability</Tags>
	</Method>
	<Method>
		<Id>116</Id>
		<Node>15</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Search Algorithms</Name>
		<Description>Algorithms designed to find solutions to problems by exploring a search space.</Description>
		<InputData>Search space, objective function</InputData>
		<OutputData>Optimal solution</OutputData>
		<WhenToUse>When finding solutions to optimization problems or searching through a space of possibilities.</WhenToUse>
		<Strengths>Finds optimal solutions in complex search spaces.</Strengths>
		<Limitations>Can be computationally expensive, may get stuck in local optima.</Limitations>
		<RelationshipToModels>Algorithms used to find optimal parameters or model structures.</RelationshipToModels>
		<EvaluationMetrics>Solution quality, search time.</EvaluationMetrics>
		<Tags>Problem Solving, Computation</Tags>
	</Method>
	<Method>
		<Id>117</Id>
		<Node>5</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Distributive Property</Name>
		<Description>A property in mathematics that relates operations on different sets or elements.</Description>
		<InputData>Mathematical expressions</InputData>
		<OutputData>Equivalent expressions</OutputData>
		<WhenToUse>When manipulating algebraic expressions.</WhenToUse>
		<Strengths>Simplifies algebraic expressions.</Strengths>
		<Limitations>Limited applicability outside of algebraic manipulation.</Limitations>
		<RelationshipToModels>A mathematical property that is not directly related to machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Correctness of simplification.</EvaluationMetrics>
		<Tags>Mathematics, Algebra</Tags>
	</Method>
	<Method>
		<Id>118</Id>
		<Node>11</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Probability and rules</Name>
		<Description>Combining probabilistic models with rule-based systems.</Description>
		<InputData>Probabilistic models, rule-based systems</InputData>
		<OutputData>Combined system output</OutputData>
		<WhenToUse>When combining probabilistic reasoning with rule-based systems.</WhenToUse>
		<Strengths>Combines probabilistic and logical reasoning.</Strengths>
		<Limitations>Complex to implement, potential for inconsistencies.</Limitations>
		<RelationshipToModels>Approaches that combine probabilistic models with rule-based systems.</RelationshipToModels>
		<EvaluationMetrics>Accuracy, consistency.</EvaluationMetrics>
		<Tags>AI, Reasoning</Tags>
	</Method>
	<Method>
		<Id>119</Id>
		<Node>12</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Markov Chain</Name>
		<Description>A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</Description>
		<InputData>States, transition probabilities</InputData>
		<OutputData>Sequence of states</OutputData>
		<WhenToUse>When modeling sequences of events where the future depends only on the current state.</WhenToUse>
		<Strengths>Models sequential data, predicts future states.</Strengths>
		<Limitations>Assumes the Markov property, may not capture long-range dependencies.</Limitations>
		<RelationshipToModels>A model used to represent sequences of events, relevant to time series analysis.</RelationshipToModels>
		<EvaluationMetrics>Transition probabilities, state distributions.</EvaluationMetrics>
		<Tags>Stochastic Processes, Time Series</Tags>
	</Method>
	<Method>
		<Id>120</Id>
		<Node>13</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Bernoulli process</Name>
		<Description>A sequence of independent Bernoulli trials, each with the same probability of success.</Description>
		<InputData>Number of trials, probability of success</InputData>
		<OutputData>Sequence of successes and failures</OutputData>
		<WhenToUse>When modeling a sequence of independent trials with binary outcomes.</WhenToUse>
		<Strengths>Models binary outcomes, calculates probabilities.</Strengths>
		<Limitations>Assumes independent trials, may not be realistic in all scenarios.</Limitations>
		<RelationshipToModels>A model of independent binary events, relevant to probability and statistics.</RelationshipToModels>
		<EvaluationMetrics>Probability of success, number of successes.</EvaluationMetrics>
		<Tags>Probability, Statistics</Tags>
	</Method>
	<Method>
		<Id>121</Id>
		<Node>14</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Random walk</Name>
		<Description>A mathematical object that describes a path that consists of a succession of random steps.</Description>
		<InputData>Starting point, step distribution</InputData>
		<OutputData>Sequence of positions</OutputData>
		<WhenToUse>When modeling processes with random steps, such as stock prices or particle movement.</WhenToUse>
		<Strengths>Models stochastic processes, simulates random movement.</Strengths>
		<Limitations>Can be unpredictable, may not converge quickly.</Limitations>
		<RelationshipToModels>A model of random movement, with applications in various fields, including finance.</RelationshipToModels>
		<EvaluationMetrics>Path characteristics, displacement.</EvaluationMetrics>
		<Tags>Stochastic Processes, Modeling</Tags>
	</Method>
	<Method>
		<Id>122</Id>
		<Node>15</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Wiener process</Name>
		<Description>A continuous-time stochastic process that models Brownian motion.</Description>
		<InputData>Time interval</InputData>
		<OutputData>Sample path</OutputData>
		<WhenToUse>When modeling continuous-time random phenomena, like Brownian motion.</WhenToUse>
		<Strengths>Models continuous-time random phenomena.</Strengths>
		<Limitations>Idealized model, real-world phenomena may deviate.</Limitations>
		<RelationshipToModels>A continuous-time version of a random walk, used in stochastic modeling.</RelationshipToModels>
		<EvaluationMetrics>Statistical properties of the process.</EvaluationMetrics>
		<Tags>Stochastic Processes, Modeling</Tags>
	</Method>
	<Method>
		<Id>123</Id>
		<Node>16</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Poisson process</Name>
		<Description>A stochastic process that models the number of events occurring in a given time interval.</Description>
		<InputData>Rate parameter, time interval</InputData>
		<OutputData>Sequence of event times or number of events</OutputData>
		<WhenToUse>When modeling the occurrence of events over time, such as customer arrivals or phone calls.</WhenToUse>
		<Strengths>Models event occurrences over time.</Strengths>
		<Limitations>Assumes events occur independently, may not hold in all cases.</Limitations>
		<RelationshipToModels>A model of the occurrence of events over time, used in reliability and queuing theory.</RelationshipToModels>
		<EvaluationMetrics>Event rate, inter-arrival times.</EvaluationMetrics>
		<Tags>Stochastic Processes, Modeling</Tags>
	</Method>
	<Method>
		<Id>124</Id>
		<Node>17</Node>
		<Lvl>2</Lvl>
		<From>11</From>
		<Name>Signal and Noise</Name>
		<Description>The distinction between meaningful information (signal) and unwanted random variations (noise) in data.</Description>
		<InputData>Data</InputData>
		<OutputData>Separated signal and noise components</OutputData>
		<WhenToUse>When analyzing data to separate meaningful information from random variations.</WhenToUse>
		<Strengths>Extracts meaningful information, removes unwanted variations.</Strengths>
		<Limitations>Difficult to perfectly separate, subjective element in the process.</Limitations>
		<RelationshipToModels>Concepts relevant to data analysis and feature engineering for machine learning models.</RelationshipToModels>
		<EvaluationMetrics>Signal-to-noise ratio, accuracy of separation.</EvaluationMetrics>
		<Tags>Data Analysis, Signal Processing</Tags>
	</Method>
	<Method>
		<Id>125</Id>
		<Node>18</Node>
		<Lvl>1</Lvl>
		<From>0</From>
		<Name>Distances (in Algorithms)</Name>
		<Description>Metrics used to quantify the similarity or dissimilarity between data points.</Description>
		<InputData>Data points</InputData>
		<OutputData>Distance measure</OutputData>
		<WhenToUse>When quantifying the similarity or dissimilarity between data points in various algorithms.</WhenToUse>
		<Strengths>Quantifies relationships, enables comparisons.</Strengths>
		<Limitations>Choice of metric can significantly impact results.</Limitations>
		<RelationshipToModels>Mathematical functions used in various machine learning algorithms.</RelationshipToModels>
		<EvaluationMetrics>Impact on algorithm performance, correlation with similarity judgments.</EvaluationMetrics>
		<Tags>Algorithms, Metrics</Tags>
	</Method>
	<Method>
		<Id>126</Id>
		<Node>19</Node>
		<Lvl>2</Lvl>
		<From>19</From>
		<Name>Manhattan</Name>
		<Description>A distance metric where the distance between two points is the sum of the absolute differences of their coordinates.</Description>
		<InputData>Two data points</InputData>
		<OutputData>Manhattan distance</OutputData>
		<WhenToUse>When distance needs to be calculated along axes at right angles.</WhenToUse>
		<Strengths>Simple to calculate, robust to outliers in some cases.</Strengths>
		<Limitations>Not rotation-invariant.</Limitations>
		<RelationshipToModels>Mathematical functions used in various machine learning algorithms.</RelationshipToModels>
		<EvaluationMetrics>Computational efficiency.</EvaluationMetrics>
		<Tags>Distance Metric</Tags>
	</Method>
	<Method>
		<Id>127</Id>
		<Node>20</Node>
		<Lvl>2</Lvl>
		<From>19</From>
		<Name>Euclidean</Name>
		<Description>The straight-line distance between two points in Euclidean space.</Description>
		<InputData>Two data points</InputData>
		<OutputData>Euclidean distance</OutputData>
		<WhenToUse>When calculating the straight-line distance between points.</WhenToUse>
		<Strengths>Represents true geometric distance.</Strengths>
		<Limitations>Sensitive to feature scaling.</Limitations>
		<RelationshipToModels>Mathematical functions used in various machine learning algorithms.</RelationshipToModels>
		<EvaluationMetrics>Accuracy in geometric applications.</EvaluationMetrics>
		<Tags>Distance Metric</Tags>
	</Method>
	<Method>
		<Id>128</Id>
		<Node>21</Node>
		<Lvl>2</Lvl>
		<From>19</From>
		<Name>Cosine</Name>
		<Description>A measure of similarity between two non-zero vectors, representing the cosine of the angle between them.</Description>
		<InputData>Two vectors</InputData>
		<OutputData>Cosine similarity</OutputData>
		<WhenToUse>When measuring the similarity between vectors, regardless of their magnitude.</WhenToUse>
		<Strengths>Measures vector similarity, insensitive to magnitude.</Strengths>
		<Limitations>Not a true distance metric.</Limitations>
		<RelationshipToModels>Mathematical functions used in various machine learning algorithms.</RelationshipToModels>
		<EvaluationMetrics>Accuracy in text and vector similarity tasks.</EvaluationMetrics>
		<Tags>Similarity Metric</Tags>
	</Method>
	<Method>
		<Id>129</Id>
		<Node>22</Node>
		<Lvl>2</Lvl>
		<From>19</From>
		<Name>Jaccard</Name>
		<Description>A measure of similarity between two sets, defined as the size of the intersection divided by the size of the union.</Description>
		<InputData>Two sets</InputData>
		<OutputData>Jaccard index</OutputData>
		<WhenToUse>When comparing the similarity of sets.</WhenToUse>
		<Strengths>Compares set similarity, useful for binary data.</Strengths>
		<Limitations>Only applicable to set data.</Limitations>
		<RelationshipToModels>Mathematical functions used in various machine learning algorithms.</RelationshipToModels>
		<EvaluationMetrics>Accuracy in set-based comparisons.</EvaluationMetrics>
		<Tags>Similarity Metric</Tags>
	</Method>
	<Method>
		<Id>130</Id>
		<Node>2</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Propositional Logic</Name>
		<Description>A branch of logic that deals with propositions and their logical relationships.</Description>
		<InputData>Propositions, logical connectives</InputData>
		<OutputData>Truth value of compound propositions</OutputData>
		<WhenToUse>When representing and reasoning about logical statements.</WhenToUse>
		<Strengths>Formalizes logical reasoning, enables truth value analysis.</Strengths>
		<Limitations>Limited expressiveness, cannot handle complex relationships.</Limitations>
		<RelationshipToModels>A system of logic that provides a foundation for some AI reasoning systems.</RelationshipToModels>
		<EvaluationMetrics>Validity of arguments, consistency of knowledge base.</EvaluationMetrics>
		<Tags>Logic, Reasoning</Tags>
	</Method>
	<Method>
		<Id>131</Id>
		<Node>7</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Inference by Resolution</Name>
		<Description>A rule of inference used in automated theorem proving.</Description>
		<InputData>Set of clauses</InputData>
		<OutputData>New clauses or proof of contradiction</OutputData>
		<WhenToUse>When automating logical reasoning and theorem proving.</WhenToUse>
		<Strengths>Automates logical deduction, proves theorems.</Strengths>
		<Limitations>Can be computationally expensive.</Limitations>
		<RelationshipToModels>A technique used in automated reasoning systems.</RelationshipToModels>
		<EvaluationMetrics>Completeness, soundness.</EvaluationMetrics>
		<Tags>Automated Reasoning, Logic</Tags>
	</Method>
	<Method>
		<Id>132</Id>
		<Node>8</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>First order logic</Name>
		<Description>A system of logic that uses quantifiers to express relationships between objects and predicates.</Description>
		<InputData>Knowledge base, query</InputData>
		<OutputData>Answer to the query</OutputData>
		<WhenToUse>When representing complex relationships between objects and properties.</WhenToUse>
		<Strengths>Expresses complex relationships, handles objects and predicates.</Strengths>
		<Limitations>Can be undecidable, complex inference rules.</Limitations>
		<RelationshipToModels>A more expressive system of logic used in knowledge representation.</RelationshipToModels>
		<EvaluationMetrics>Expressiveness, correctness of inferences.</EvaluationMetrics>
		<Tags>Logic, Knowledge Representation</Tags>
	</Method>
	<Method>
		<Id>133</Id>
		<Node>9</Node>
		<Lvl>2</Lvl>
		<From>2</From>
		<Name>Quantification, universal, existential</Name>
		<Description>Logical operators that specify the quantity of individuals that satisfy a predicate (universal: 'for all,' existential: 'there exists').</Description>
		<InputData>Predicate, domain of discourse</InputData>
		<OutputData>Truth value of quantified statement</OutputData>
		<WhenToUse>When expressing statements about all or some objects in a domain.</WhenToUse>
		<Strengths>Expresses general and specific statements, enables precise reasoning.</Strengths>
		<Limitations>Can be difficult to reason with in complex domains.</Limitations>
		<RelationshipToModels>A more expressive system of logic used in knowledge representation.</RelationshipToModels>
		<EvaluationMetrics>Correctness of logical statements.</EvaluationMetrics>
		<Tags>Logic, Semantics</Tags>
	</Method>
</Methods>